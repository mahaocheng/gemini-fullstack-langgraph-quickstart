{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 确保环境变量已加载 (特别是 GEMINI_API_KEY)\n",
    "# 假设 .env 文件位于 notebook 所在的 backend 目录的上一级 (即项目根目录)\n",
    "# 如果 .env 在 backend 目录中，路径应为 '.env' 或 '../.env' (如果notebook在src/agent下)\n",
    "# 根据您的 .env 文件实际位置调整 dotenv_path\n",
    "# 您之前提到 .env 在 backend 目录中，所以应该是 load_dotenv() 或 load_dotenv(dotenv_path='.env')\n",
    "# 如果 .env 在项目根目录，则是 load_dotenv(dotenv_path='../.env')\n",
    "# 从您打开的文件看，.env 在 backend 目录，所以 load_dotenv() 应该可以。\n",
    "if load_dotenv(): # 或者 load_dotenv(dotenv_path='.env')\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Could not load .env file, ensure GEMINI_API_KEY is set in your environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set: True\n",
      "LANGSMITH_API_KEY is set: True\n",
      "GEMINI_API_KEY is : AIzaSyBPRZjGecVeND_KqHac8jeebFDzvlxFQhs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 打印一下关键环境变量，确认是否加载成功\n",
    "print(f\"GEMINI_API_KEY is set: {bool(os.getenv('GEMINI_API_KEY'))}\")\n",
    "print(f\"LANGSMITH_API_KEY is set: {bool(os.getenv('LANGSMITH_API_KEY'))}\") \n",
    "print(f\"GEMINI_API_KEY is : {os.getenv('GEMINI_API_KEY')}\") # 如果 LangSmith 也需要配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I help you today?' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_07e970ab25'} id='run--7d047d5e-e9df-4ad2-9ff6-0385fffb280e-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import os, dotenv, pathlib\n",
    "\n",
    "# 加载 .env（若在 backend 目录，可写 dotenv.load_dotenv(\"backend/.env\")）\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# 创建 LLM 实例\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name='gpt-4.1',\n",
    "    model_name='gpt-4.1',\n",
    "    temperature=0.2,\n",
    "    streaming=True,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = llm.invoke([HumanMessage(content=\"hello\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Client in module google.genai.client object:\n",
      "\n",
      "class Client(builtins.object)\n",
      " |  Client(*, vertexai: Optional[bool] = None, api_key: Optional[str] = None, credentials: Optional[google.auth.credentials.Credentials] = None, project: Optional[str] = None, location: Optional[str] = None, debug_config: Optional[google.genai.client.DebugConfig] = None, http_options: Union[google.genai.types.HttpOptions, google.genai.types.HttpOptionsDict, NoneType] = None)\n",
      " |  \n",
      " |  Client for making synchronous requests.\n",
      " |  \n",
      " |  Use this client to make a request to the Gemini Developer API or Vertex AI\n",
      " |  API and then wait for the response.\n",
      " |  \n",
      " |  To initialize the client, provide the required arguments either directly\n",
      " |  or by using environment variables. Gemini API users and Vertex AI users in\n",
      " |  express mode can provide API key by providing input argument\n",
      " |  `api_key=\"your-api-key\"` or by defining `GOOGLE_API_KEY=\"your-api-key\"` as an\n",
      " |  environment variable\n",
      " |  \n",
      " |  Vertex AI API users can provide inputs argument as `vertexai=True,\n",
      " |  project=\"your-project-id\", location=\"us-central1\"` or by defining\n",
      " |  `GOOGLE_GENAI_USE_VERTEXAI=true`, `GOOGLE_CLOUD_PROJECT` and\n",
      " |  `GOOGLE_CLOUD_LOCATION` environment variables.\n",
      " |  \n",
      " |  Attributes:\n",
      " |    api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to\n",
      " |      use for authentication. Applies to the Gemini Developer API only.\n",
      " |    vertexai: Indicates whether the client should use the Vertex AI API\n",
      " |      endpoints. Defaults to False (uses Gemini Developer API endpoints).\n",
      " |      Applies to the Vertex AI API only.\n",
      " |    credentials: The credentials to use for authentication when calling the\n",
      " |      Vertex AI APIs. Credentials can be obtained from environment variables and\n",
      " |      default credentials. For more information, see `Set up Application Default\n",
      " |      Credentials\n",
      " |      <https://cloud.google.com/docs/authentication/provide-credentials-adc>`_.\n",
      " |      Applies to the Vertex AI API only.\n",
      " |    project: The `Google Cloud project ID\n",
      " |      <https://cloud.google.com/vertex-ai/docs/start/cloud-environment>`_ to use\n",
      " |      for quota. Can be obtained from environment variables (for example,\n",
      " |      ``GOOGLE_CLOUD_PROJECT``). Applies to the Vertex AI API only.\n",
      " |      Find your `Google Cloud project ID <https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects>`_.\n",
      " |    location: The `location\n",
      " |      <https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations>`_\n",
      " |      to send API requests to (for example, ``us-central1``). Can be obtained\n",
      " |      from environment variables. Applies to the Vertex AI API only.\n",
      " |    debug_config: Config settings that control network behavior of the client.\n",
      " |      This is typically used when running test code.\n",
      " |    http_options: Http options to use for the client. These options will be\n",
      " |      applied to all requests made by the client. Example usage: `client =\n",
      " |      genai.Client(http_options=types.HttpOptions(api_version='v1'))`.\n",
      " |  \n",
      " |  Usage for the Gemini Developer API:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |    from google import genai\n",
      " |  \n",
      " |    client = genai.Client(api_key='my-api-key')\n",
      " |  \n",
      " |  Usage for the Vertex AI API:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |    from google import genai\n",
      " |  \n",
      " |    client = genai.Client(\n",
      " |        vertexai=True, project='my-project-id', location='us-central1'\n",
      " |    )\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, vertexai: Optional[bool] = None, api_key: Optional[str] = None, credentials: Optional[google.auth.credentials.Credentials] = None, project: Optional[str] = None, location: Optional[str] = None, debug_config: Optional[google.genai.client.DebugConfig] = None, http_options: Union[google.genai.types.HttpOptions, google.genai.types.HttpOptionsDict, NoneType] = None)\n",
      " |      Initializes the client.\n",
      " |      \n",
      " |      Args:\n",
      " |         vertexai (bool): Indicates whether the client should use the Vertex AI\n",
      " |           API endpoints. Defaults to False (uses Gemini Developer API endpoints).\n",
      " |           Applies to the Vertex AI API only.\n",
      " |         api_key (str): The `API key\n",
      " |           <https://ai.google.dev/gemini-api/docs/api-key>`_ to use for\n",
      " |           authentication. Applies to the Gemini Developer API only.\n",
      " |         credentials (google.auth.credentials.Credentials): The credentials to use\n",
      " |           for authentication when calling the Vertex AI APIs. Credentials can be\n",
      " |           obtained from environment variables and default credentials. For more\n",
      " |           information, see `Set up Application Default Credentials\n",
      " |           <https://cloud.google.com/docs/authentication/provide-credentials-adc>`_.\n",
      " |           Applies to the Vertex AI API only.\n",
      " |         project (str): The `Google Cloud project ID\n",
      " |           <https://cloud.google.com/vertex-ai/docs/start/cloud-environment>`_ to\n",
      " |           use for quota. Can be obtained from environment variables (for example,\n",
      " |           ``GOOGLE_CLOUD_PROJECT``). Applies to the Vertex AI API only.\n",
      " |         location (str): The `location\n",
      " |           <https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations>`_\n",
      " |           to send API requests to (for example, ``us-central1``). Can be obtained\n",
      " |           from environment variables. Applies to the Vertex AI API only.\n",
      " |         debug_config (DebugConfig): Config settings that control network behavior\n",
      " |           of the client. This is typically used when running test code.\n",
      " |         http_options (Union[HttpOptions, HttpOptionsDict]): Http options to use\n",
      " |           for the client.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  aio\n",
      " |  \n",
      " |  auth_tokens\n",
      " |  \n",
      " |  batches\n",
      " |  \n",
      " |  caches\n",
      " |  \n",
      " |  chats\n",
      " |  \n",
      " |  files\n",
      " |  \n",
      " |  models\n",
      " |  \n",
      " |  operations\n",
      " |  \n",
      " |  tunings\n",
      " |  \n",
      " |  vertexai\n",
      " |      Returns whether the client is using the Vertex AI API.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai import Client\n",
    "\n",
    "genai_client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "help(genai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.DEBUG\n",
    ")\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in Configuration class:\n",
      "- query_generator_model: (default: gemini-2.0-flash)\n",
      "- reflection_model: (default: gemini-2.5-flash-preview-04-17)\n",
      "- answer_model: (default: gemini-2.5-pro-preview-05-06)\n",
      "- openai_query_generator_model: (default: gpt-4o-mini)\n",
      "- openai_reflection_model: (default: o1-preview)\n",
      "- openai_answer_model: (default: gpt-4.1)\n",
      "- number_of_initial_queries: (default: 3)\n",
      "- max_research_loops: (default: 2)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Created Configuration instance with current settings:\n",
      "{\n",
      "  \"query_generator_model\": \"gemini-2.0-flash\",\n",
      "  \"reflection_model\": \"gemini-2.5-flash-preview-04-17\",\n",
      "  \"answer_model\": \"gemini-2.5-pro-preview-05-06\",\n",
      "  \"openai_query_generator_model\": \"gpt-4o-mini\",\n",
      "  \"openai_reflection_model\": \"o1-preview\",\n",
      "  \"openai_answer_model\": \"gpt-4.1\",\n",
      "  \"number_of_initial_queries\": 3,\n",
      "  \"max_research_loops\": 2\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Path from where 'src.agent.configuration' was loaded:\n",
      "d:\\mahc05\\gemini-fullstack-langgraph-quickstart\\backend\\src\\agent\\configuration.py\n"
     ]
    }
   ],
   "source": [
    "from src.agent.graph import graph\n",
    "from src.agent.configuration import Configuration\n",
    "import inspect\n",
    "\n",
    "# 打印 Configuration 类的所有字段及其默认值\n",
    "print(\"Fields in Configuration class:\")\n",
    "for field_name, field_obj in Configuration.model_fields.items():\n",
    "    print(f\"- {field_name}: (default: {field_obj.default})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 尝试创建一个 Configuration 实例并打印其内容\n",
    "# 这也会验证 from_runnable_config 是否能正常工作（如果环境变量未设置，则使用默认值）\n",
    "try:\n",
    "    config_instance = Configuration.from_runnable_config()\n",
    "    print(\"Created Configuration instance with current settings:\")\n",
    "    print(config_instance.model_dump_json(indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Configuration instance: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 检查模块的加载路径\n",
    "print(\"Path from where 'src.agent.configuration' was loaded:\")\n",
    "print(inspect.getfile(Configuration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'beta.chat.completions.stream', 'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d842132b-a1cc-452f-a52f-a271992fd00b', 'json_data': {'messages': [{'content': 'Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\\n\\nInstructions:\\n- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\\n- Each query should focus on one specific aspect of the original question.\\n- Don\\'t produce more than 1 queries.\\n- Queries should be diverse, if the topic is broad, generate more than 1 query.\\n- Don\\'t generate multiple similar queries, 1 is enough.\\n- Query should ensure that the most current information is gathered. The current date is June 13, 2025.\\n\\nFormat: \\n- Format your response as a JSON object with ALL three of these exact keys:\\n   - \"rationale\": Brief explanation of why these queries are relevant\\n   - \"query\": A list of search queries\\n\\nExample:\\n\\nTopic: What revenue grew more last year apple stock or the number of people buying an iphone\\n```json\\n{\\n    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple\\'s stock performance and iPhone sales metrics. These queries target the precise financial information needed: company revenue trends, product-specific unit sales figures, and stock price movement over the same fiscal period for direct comparison.\",\\n    \"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\\n}\\n```\\n\\nContext: 介绍马斯克的公司', 'role': 'user'}], 'model': 'gpt-4o-mini', 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'query': {'description': 'A list of search queries to be used for web research.', 'items': {'type': 'string'}, 'title': 'Query', 'type': 'array'}, 'rationale': {'description': 'A brief explanation of why these queries are relevant to the research topic.', 'title': 'Rationale', 'type': 'string'}}, 'required': ['query', 'rationale'], 'title': 'SearchQueryList', 'type': 'object', 'additionalProperties': False}, 'name': 'SearchQueryList', 'strict': True}}, 'stream': True, 'temperature': 1.0}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'beta.chat.completions.stream', 'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-d842132b-a1cc-452f-a52f-a271992fd00b', 'json_data': {'messages': [{'content': 'Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\\n\\nInstructions:\\n- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\\n- Each query should focus on one specific aspect of the original question.\\n- Don\\'t produce more than 1 queries.\\n- Queries should be diverse, if the topic is broad, generate more than 1 query.\\n- Don\\'t generate multiple similar queries, 1 is enough.\\n- Query should ensure that the most current information is gathered. The current date is June 13, 2025.\\n\\nFormat: \\n- Format your response as a JSON object with ALL three of these exact keys:\\n   - \"rationale\": Brief explanation of why these queries are relevant\\n   - \"query\": A list of search queries\\n\\nExample:\\n\\nTopic: What revenue grew more last year apple stock or the number of people buying an iphone\\n```json\\n{\\n    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple\\'s stock performance and iPhone sales metrics. These queries target the precise financial information needed: company revenue trends, product-specific unit sales figures, and stock price movement over the same fiscal period for direct comparison.\",\\n    \"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\\n}\\n```\\n\\nContext: 介绍马斯克的公司', 'role': 'user'}], 'model': 'gpt-4o-mini', 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'query': {'description': 'A list of search queries to be used for web research.', 'items': {'type': 'string'}, 'title': 'Query', 'type': 'array'}, 'rationale': {'description': 'A brief explanation of why these queries are relevant to the research topic.', 'title': 'Rationale', 'type': 'string'}}, 'required': ['query', 'rationale'], 'title': 'SearchQueryList', 'type': 'object', 'additionalProperties': False}, 'name': 'SearchQueryList', 'strict': True}}, 'stream': True, 'temperature': 1.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\n",
      "Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED9AFC0B10>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED9AFC0B10>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001ED97D34440> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x000001ED97D34440> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED9AF4CE90>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED9AF4CE90>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Fri, 13 Jun 2025 15:47:06 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Fri, 13 Jun 2025 15:47:06 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "INFO:httpx:HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Fri, 13 Jun 2025 15:47:06 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Fri, 13 Jun 2025 15:47:06 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "AFC is enabled with max remote calls: 10.\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED96206690>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED96206690>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001ED97C6F140> server_hostname='generativelanguage.googleapis.com' timeout=None\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x000001ED97C6F140> server_hostname='generativelanguage.googleapis.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED9938C0D0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001ED9938C0D0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Fri, 13 Jun 2025 15:47:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=74'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Fri, 13 Jun 2025 15:47:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=74'), (b'Alt-Svc', b'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])\n",
      "INFO:httpx:HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent \"HTTP/1.1 429 Too Many Requests\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m state = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m介绍马斯克的公司\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_research_loops\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitial_search_query_count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitial_search_query_count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\mahc05\\gemini-fullstack-langgraph-quickstart\\backend\\src\\agent\\graph.py:114\u001b[39m, in \u001b[36mweb_research\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    110\u001b[39m query = state[\u001b[33m\"\u001b[39m\u001b[33msearch_query\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Use the Google Search tool to get the search results\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Ensure a valid Gemini model name is passed for tool execution.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m tool_result = \u001b[43mgenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels/gemini-1.5-flash-latest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweb_searcher_instructions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresearch_topic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_current_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m  \u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle_search\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Extract the search results from the tool call\u001b[39;00m\n\u001b[32m    124\u001b[39m response = tool_result.candidates[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\models.py:6058\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   6056\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   6057\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6058\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6059\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   6060\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6061\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   6062\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\models.py:5007\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5004\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   5005\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m5007\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5008\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   5012\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   5013\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   5014\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\_api_client.py:927\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    918\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    919\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    923\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    924\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    925\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    926\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m   json_response = response.json\n\u001b[32m    929\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\_api_client.py:793\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    786\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m    787\u001b[39m       method=http_request.method,\n\u001b[32m    788\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    791\u001b[39m       timeout=http_request.timeout,\n\u001b[32m    792\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    795\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    796\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\errors.py:104\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    102\u001b[39m status_code = response.status_code\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    106\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}]}}",
      "During task with name 'web_research' and id 'ab17d67e-f31f-6417-1822-0e86c193c12d'"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"介绍特朗普\"}], \"max_research_loops\": 3, \"initial_search_query_count\": 3, \"initial_search_query_count\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.genai import Client\n",
    "\n",
    "# Initialize the Gemini client\n",
    "genai_client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Test a simple query\n",
    "print(os.getenv(\"GEMINI_API_KEY\"))\n",
    "response = genai_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"hello\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pprint import pprint # For pretty printing dictionaries\n",
    "\n",
    "# 确保环境变量已加载 (特别是 GEMINI_API_KEY)\n",
    "if load_dotenv(): # 或者 load_dotenv(dotenv_path='.env')\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Could not load .env file, ensure GEMINI_API_KEY is set in your environment\")\n",
    "\n",
    "print(f\"GEMINI_API_KEY is set: {bool(os.getenv('GEMINI_API_KEY'))}\")\n",
    "print(f\"LANGSMITH_API_KEY is set: {bool(os.getenv('LANGSMITH_API_KEY'))}\")\n",
    "\n",
    "try:\n",
    "    from src.agent.graph import graph\n",
    "    print(\"Successfully imported 'graph' from src.agent.graph\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing graph: {e}\")\n",
    "    graph = None\n",
    "\n",
    "async def main():\n",
    "    if not graph:\n",
    "        print(\"Graph not loaded, skipping invocation.\")\n",
    "        return\n",
    "\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Who won the UEFA Euro 2024 football tournament?\"}\n",
    "        ],\n",
    "        \"max_research_loops\": 2,\n",
    "        \"initial_search_query_count\": 2\n",
    "    }\n",
    "\n",
    "    run_config = {\n",
    "        \"configurable\": {\n",
    "            # \"query_generator_model\": \"gemini-1.5-flash-latest\",\n",
    "            # \"reasoning_model\": \"gemini-1.5-pro-latest\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nStreaming agent execution log with initial state:\")\n",
    "    print(initial_state)\n",
    "    if run_config.get(\"configurable\"):\n",
    "        print(\"\\nAnd config:\")\n",
    "        print(run_config)\n",
    "    print(\"\\n--- Agent Log Start ---\")\n",
    "\n",
    "    try:\n",
    "        # 使用 astream_log() 来获取详细的执行日志\n",
    "        async for log_patch in graph.astream_log(\n",
    "            initial_state, \n",
    "            config=run_config, \n",
    "            include_names=None, \n",
    "            include_types=None, \n",
    "        ):\n",
    "            # RunLogPatch 对象包含一个 'ops' 列表，每个 op 是一个操作描述\n",
    "            for op in log_patch.ops:\n",
    "                print(f\"\\n[Log Operation: {op['op']}] Path: {op['path']}\")\n",
    "                if op['op'] == \"add\" or op['op'] == \"replace\":\n",
    "                    # 'value' 字段包含了添加或替换的数据\n",
    "                    print(\"  Value:\")\n",
    "                    pprint(op['value'], indent=2, width=120)\n",
    "                elif op['op'] == \"remove\":\n",
    "                    print(f\"  (Removed path: {op['path']})\")\n",
    "                # 可以根据 op['op'] 的类型 ('add', 'remove', 'replace') 和 op['path'] 来决定如何展示信息\n",
    "                # 例如，关心特定路径的更新：\n",
    "                # if op['path'] == \"/logs/OverallState/final_output\" and op['op'] == 'add':\n",
    "                #     print(\">>> Final Output Updated <<<\")\n",
    "                #     pprint(op['value'])\n",
    "                # if \"streamed_output\" in op['path'] and op['op'] == 'add': # LLM token stream\n",
    "                #     if isinstance(op['value'], list) and op['value'] and hasattr(op['value'][0], 'content'):\n",
    "                #         print(f\"  LLM Stream: {op['value'][0].content}\", end=\"\") # 假设是 AIMessageChunk\n",
    "                #     else:\n",
    "                #         pprint(op['value'], indent=2)\n",
    "\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        print(\"\\n--- Agent Log End ---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during agent streaming: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# **在 Notebook 单元格中，请直接运行:**\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({\"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": \"How has the most titles? List the top 5\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本，用于单独测试 generate_query 函数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.agent.graph import generate_query\n",
    "from src.agent.configuration import Configuration\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 创建测试状态\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建配置\n",
    "test_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"query_generator_model\": \"gemini-2.0-flash\",\n",
    "        \"reasoning_model\": \"gemini-2.0-pro\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 测试 generate_query 函数\n",
    "try:\n",
    "    print(\"\\n开始测试 generate_query 函数...\")\n",
    "    result = generate_query(test_state, test_config)\n",
    "    print(\"\\n测试成功!\")\n",
    "    print(f\"生成的查询: {result.get('search_query', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test script to directly test the Gemini API without importing from the project modules.\n",
    "This will help us verify if the issue is with our code changes or with the Gemini API itself.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key is set\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY is not set\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY is set\")\n",
    "\n",
    "# Import the Gemini API client\n",
    "try:\n",
    "    from google.genai import Client\n",
    "    print(\"Successfully imported google.genai\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing google.genai: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the Gemini API client\n",
    "genai_client = Client(api_key=gemini_api_key)\n",
    "\n",
    "# Test a simple query with system instruction for structured output\n",
    "try:\n",
    "    print(\"\\nTesting Gemini API with system instruction for structured output...\")\n",
    "    \n",
    "    # Define the system instruction for structured output\n",
    "    system_instruction = (\n",
    "        \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "        \"Return your response as a JSON object with the following structure: \"\n",
    "        \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "        \"where each query is a string that would be effective for web search.\"\n",
    "    )\n",
    "    \n",
    "    # Define the user prompt\n",
    "    user_prompt = \"Generate search queries for: Who won the UEFA Euro 2024 football tournament?\"\n",
    "    \n",
    "    # Set the generation config\n",
    "    generation_config = {\n",
    "        \"temperature\": 1.0,\n",
    "    }\n",
    "    \n",
    "    # Call the Gemini API\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[\n",
    "            {\"role\": \"system\", \"parts\": [system_instruction]},\n",
    "            {\"role\": \"user\", \"parts\": [user_prompt]}\n",
    "        ],\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    \n",
    "    # Print the response\n",
    "    print(f\"Response text: {response.text}\")\n",
    "    \n",
    "    # Try to parse the JSON response\n",
    "    try:\n",
    "        # Find the JSON part in the response\n",
    "        response_text = response.text\n",
    "        json_start = response_text.find('{')\n",
    "        json_end = response_text.rfind('}')\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_str = response_text[json_start:json_end+1]\n",
    "            data = json.loads(json_str)\n",
    "            print(f\"Parsed JSON: {json.dumps(data, indent=2)}\")\n",
    "        else:\n",
    "            print(\"No JSON found in response\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    \n",
    "    print(\"Test completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Gemini API: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "测试不同的 Gemini API 调用格式\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 初始化 Gemini 客户端\n",
    "#genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# 创建客户端\n",
    "client = genai.Client()\n",
    "# 测试不同的格式\n",
    "def test_format1():\n",
    "    \"\"\"测试格式1: 使用 Content 对象\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式1: 使用 Content 对象\")\n",
    "        \n",
    "        # 创建 Content 对象\n",
    "        content = genai.Content(\n",
    "            parts=[\n",
    "                genai.Part(text=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "            ],\n",
    "            role=\"user\"\n",
    "        )\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=content\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format2():\n",
    "    \"\"\"测试格式2: 使用字符串\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式2: 使用字符串\")\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=\"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format3():\n",
    "    \"\"\"测试格式3: 使用字典列表\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式3: 使用字典列表\")\n",
    "        \n",
    "        # 系统指令\n",
    "        system_instruction = (\n",
    "            \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "            \"Return your response as a JSON object with the following structure: \"\n",
    "            \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "            \"where each query is a string that would be effective for web search.\"\n",
    "        )\n",
    "        \n",
    "        # 用户提示\n",
    "        user_prompt = \"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                {\"role\": \"system\", \"parts\": [{\"text\": system_instruction}]},\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format4():\n",
    "    \"\"\"测试格式4: 使用 Part 对象列表\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式4: 使用 Part 对象列表\")\n",
    "        \n",
    "        # 系统指令\n",
    "        system_instruction = (\n",
    "            \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "            \"Return your response as a JSON object with the following structure: \"\n",
    "            \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "            \"where each query is a string that would be effective for web search.\"\n",
    "        )\n",
    "        \n",
    "        # 用户提示\n",
    "        user_prompt = \"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        \n",
    "        # 创建 Content 对象\n",
    "        system_content = genai.Content(\n",
    "            parts=[genai.Part(text=system_instruction)],\n",
    "            role=\"system\"\n",
    "        )\n",
    "        \n",
    "        user_content = genai.Content(\n",
    "            parts=[genai.Part(text=user_prompt)],\n",
    "            role=\"user\"\n",
    "        )\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[system_content, user_content]\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"开始测试不同的 Gemini API 调用格式...\")\n",
    "\n",
    "results = []\n",
    "results.append((\"格式1\", test_format1()))\n",
    "results.append((\"格式2\", test_format2()))\n",
    "results.append((\"格式3\", test_format3()))\n",
    "results.append((\"格式4\", test_format4()))\n",
    "\n",
    "print(\"\\n测试结果汇总:\")\n",
    "for format_name, success in results:\n",
    "    status = \"成功\" if success else \"失败\"\n",
    "    print(f\"{format_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本，用于单独测试 generate_query 函数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 导入要测试的函数\n",
    "from src.agent.graph import generate_query\n",
    "from src.agent.configuration import Configuration\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 创建测试状态\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个模拟的 Configuration 类\n",
    "class MockConfiguration:\n",
    "    def __init__(self):\n",
    "        self.query_generator_model = \"gemini-2.0-flash\"\n",
    "        self.reasoning_model = \"gemini-2.0-pro\"\n",
    "        self.number_of_initial_queries = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def from_runnable_config(cls, config):\n",
    "        return cls()\n",
    "\n",
    "# 创建一个模拟的 RunnableConfig 类\n",
    "class MockRunnableConfig:\n",
    "    def __init__(self):\n",
    "        self.configurable = MockConfiguration()\n",
    "\n",
    "# 测试 generate_query 函数\n",
    "try:\n",
    "    print(\"\\n开始测试 generate_query 函数...\")\n",
    "    print(\"\\n使用的模型: gemini-2.0-flash\")\n",
    "    print(\"\\n测试状态: \", test_state)\n",
    "    \n",
    "    # 添加调试信息\n",
    "    print(\"\\n开始调用 generate_query...\")\n",
    "    \n",
    "    # 创建模拟的配置对象\n",
    "    test_config = MockRunnableConfig()\n",
    "    \n",
    "    # 调用函数\n",
    "    result = generate_query(test_state, test_config)\n",
    "    print(\"\\n测试成功!\")\n",
    "    print(f\"生成的查询: {result.get('query_list', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os, dotenv\n",
    "\n",
    "# 加载 .env（若在 backend 目录，可写 dotenv.load_dotenv(\"backend/.env\")）\n",
    "dotenv.load_dotenv()\n",
    "# 创建 LLM 实例\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name='gpt-4.1',\n",
    "    model_name='gpt-4.1',\n",
    "    temperature=0.2,\n",
    "    streaming=True,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = llm.invoke([HumanMessage(content=\"hello\")])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
