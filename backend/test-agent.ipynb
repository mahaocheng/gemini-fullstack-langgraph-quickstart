{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env file\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 确保环境变量已加载 (特别是 GEMINI_API_KEY)\n",
    "# 假设 .env 文件位于 notebook 所在的 backend 目录的上一级 (即项目根目录)\n",
    "# 如果 .env 在 backend 目录中，路径应为 '.env' 或 '../.env' (如果notebook在src/agent下)\n",
    "# 根据您的 .env 文件实际位置调整 dotenv_path\n",
    "# 您之前提到 .env 在 backend 目录中，所以应该是 load_dotenv() 或 load_dotenv(dotenv_path='.env')\n",
    "# 如果 .env 在项目根目录，则是 load_dotenv(dotenv_path='../.env')\n",
    "# 从您打开的文件看，.env 在 backend 目录，所以 load_dotenv() 应该可以。\n",
    "if load_dotenv(): # 或者 load_dotenv(dotenv_path='.env')\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Could not load .env file, ensure GEMINI_API_KEY is set in your environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "DEFAULT_SEARCH_ENGINE_TIMEOUT = 100\n",
    "REFERENCE_COUNT = 5\n",
    "GOOGLE_SEARCH_ENDPOINT = \"https://customsearch.googleapis.com/customsearch/v1\"\n",
    "\n",
    "def search_with_google(query: str, subscription_key: str, cx: str):\n",
    "    \"\"\"\n",
    "    Search with google and return the contexts.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"key\": subscription_key,\n",
    "        \"cx\": cx,\n",
    "        \"q\": query,\n",
    "        \"num\": REFERENCE_COUNT,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        GOOGLE_SEARCH_ENDPOINT, params=params, timeout=DEFAULT_SEARCH_ENGINE_TIMEOUT\n",
    "    )\n",
    "    if not response.ok:\n",
    "        print(f\"{response.status_code} {response.text}\")\n",
    "        raise HTTPException(response.status_code, \"Search engine error.\")\n",
    "    json_content = response.json()\n",
    "    try:\n",
    "        contexts = json_content[\"items\"][:REFERENCE_COUNT]\n",
    "    except KeyError:\n",
    "        print(f\"Error encountered: {json_content}\")\n",
    "        return []\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 {\n",
      "  \"error\": {\n",
      "    \"code\": 429,\n",
      "    \"message\": \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:54920743327'.\",\n",
      "    \"errors\": [\n",
      "      {\n",
      "        \"message\": \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:54920743327'.\",\n",
      "        \"domain\": \"global\",\n",
      "        \"reason\": \"rateLimitExceeded\"\n",
      "      }\n",
      "    ],\n",
      "    \"status\": \"RESOURCE_EXHAUSTED\",\n",
      "    \"details\": [\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.ErrorInfo\",\n",
      "        \"reason\": \"RATE_LIMIT_EXCEEDED\",\n",
      "        \"domain\": \"googleapis.com\",\n",
      "        \"metadata\": {\n",
      "          \"quota_limit\": \"DefaultPerDayPerProject\",\n",
      "          \"quota_metric\": \"customsearch.googleapis.com/requests\",\n",
      "          \"service\": \"customsearch.googleapis.com\",\n",
      "          \"quota_unit\": \"1/d/{project}\",\n",
      "          \"consumer\": \"projects/54920743327\",\n",
      "          \"quota_location\": \"global\",\n",
      "          \"quota_limit_value\": \"100\"\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n",
      "        \"links\": [\n",
      "          {\n",
      "            \"description\": \"Request a higher quota limit.\",\n",
      "            \"url\": \"https://cloud.google.com/docs/quotas/help/request_increase\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'HTTPException' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m search_api_key = os.environ[\u001b[33m\"\u001b[39m\u001b[33mGOOGLE_SEARCH_API_KEY\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# with DDGS() as ddgs:\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     print(query, \"xxx\")\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#     results = list(ddgs.text(query, max_results=5))\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[43msearch_with_google\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m中国的首都\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_api_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGOOGLE_SEARCH_CX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36msearch_with_google\u001b[39m\u001b[34m(query, subscription_key, cx)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.ok:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mHTTPException\u001b[49m(response.status_code, \u001b[33m\"\u001b[39m\u001b[33mSearch engine error.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m json_content = response.json()\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'HTTPException' is not defined"
     ]
    }
   ],
   "source": [
    "search_api_key = os.environ[\"GOOGLE_SEARCH_API_KEY\"]\n",
    "# with DDGS() as ddgs:\n",
    "#     print(query, \"xxx\")\n",
    "#     results = list(ddgs.text(query, max_results=5))\n",
    "\n",
    "results = search_with_google(\"中国的首都\", search_api_key, \n",
    "os.environ[\"GOOGLE_SEARCH_CX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'kind': 'customsearch#result', 'title': '中国首都- 维基百科，自由的百科全书', 'htmlTitle': '<b>中国首都</b>- 维基百科，自由的百科全书', 'link': 'https://zh.wikipedia.org/zh-hans/%E4%B8%AD%E5%9B%BD%E9%A6%96%E9%83%BD', 'displayLink': 'zh.wikipedia.org', 'snippet': '华东 · 绍兴：越国首都、南明鲁监国前期首都 · 舟山：南明鲁监国后期首都 · 温州：东瓯首都 · 苏州：吴国首都、越国首都、孙吴首都、张士诚政权首都 · 淄博：齐国首都 · 寿县：楚\\xa0...', 'htmlSnippet': '华东 &middot; 绍兴：越国<b>首都</b>、南明鲁监国前期<b>首都</b> &middot; 舟山：南明鲁监国后期<b>首都</b> &middot; 温州：东瓯<b>首都</b> &middot; 苏州：吴国<b>首都</b>、越国<b>首都</b>、孙吴<b>首都</b>、张士诚政权<b>首都</b> &middot; 淄博：齐国<b>首都</b> &middot; 寿县：楚&nbsp;...', 'formattedUrl': 'https://zh.wikipedia.org/zh-hans/中国首都', 'htmlFormattedUrl': 'https://zh.wikipedia.org/zh-hans/<b>中国首都</b>', 'pagemap': {'metatags': [{'referrer': 'origin', 'og:image': 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/China_satellite.png/1200px-China_satellite.png', 'theme-color': '#eaecf0', 'og:image:width': '1200', 'og:type': 'website', 'viewport': 'width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=0.25, maximum-scale=5.0', 'og:title': '中国首都 - 维基百科，自由的百科全书', 'og:image:height': '898', 'format-detection': 'telephone=no'}]}}, {'kind': 'customsearch#result', 'title': '中华人民共和国首都_中华人民共和国中央人民政府门户网站', 'htmlTitle': '中华人民共和国<b>首都</b>_中华人民共和国中央人民政府门户网站', 'link': 'https://www.gov.cn/guoqing/2005-05/24/content_2615214.htm', 'displayLink': 'www.gov.cn', 'snippet': '北京，简称京，是中国共产党中央委员会、中华人民共和国中央人民政府所在地。中央四个直辖市之一，是全国政治、经济和科学文化的中心，也是国内国际交往的中心之一，是中国历史\\xa0...', 'htmlSnippet': '北京，简称京，是<b>中国</b>共产党中央委员会、中华人民共和国中央人民政府所在地。中央四个直辖市之一，是全国政治、经济和科学文化的中心，也是国内国际交往的中心之一，是<b>中国</b>历史&nbsp;...', 'formattedUrl': 'https://www.gov.cn/guoqing/2005-05/24/content_2615214.htm', 'htmlFormattedUrl': 'https://www.gov.cn/guoqing/2005-05/24/content_2615214.htm'}, {'kind': 'customsearch#result', 'title': '北京概况_首都之窗_北京市人民政府门户网站', 'htmlTitle': '北京概况_<b>首都</b>之窗_北京市人民政府门户网站', 'link': 'https://www.beijing.gov.cn/renwen/bjgk/', 'displayLink': 'www.beijing.gov.cn', 'snippet': '北京，简称“京”，是中华人民共和国的首都，是全国的政治中心、文化中心，是世界著名古都和现代化国际城市。北京位于北纬39度56分、东经116度20分，地处华北大平原的北部，东\\xa0...', 'htmlSnippet': '北京，简称“京”，是中华人民共和国的<b>首都</b>，是全国的政治中心、文化中心，是世界著名古都和现代化国际城市。北京位于北纬39度56分、东经116度20分，地处华北大平原的北部，东&nbsp;...', 'formattedUrl': 'https://www.beijing.gov.cn/renwen/bjgk/', 'htmlFormattedUrl': 'https://www.beijing.gov.cn/renwen/bjgk/', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT4TGkNggOP_UJG2RtHRtOr_qU9TzLOyoHzE5F0AdUvQivV9OFJVUujjD-_&s', 'width': '187', 'height': '270'}], 'metatags': [{'columndescription': '北京，简称“京”，是中华人民共和国的首都，是全国的政治中心、文化中心，是世界著名古都和现代化国际城市。北京位于北纬39度56分、东经116度20分，地处华北大平原的北部，东面与天津市毗连，其余均与河北省相邻。', 'viewport': 'width=device-width, initial-scale=1.0', 'siteidcode': '1100000088', 'columnkeywords': '国际城市;文化中心;古都;北部;首都;政治中心;大平原;现代化', 'applicable-device': 'pc,mobile', 'sitename': '首都之窗_北京市人民政府门户网站', 'sitedomain': 'www.beijing.gov.cn', 'columnname': '北京概况', 'columntype': '北京概况'}], 'cse_image': [{'src': 'https://www.beijing.gov.cn/images/imgzs0_xss_20200821.jpg'}]}}, {'kind': 'customsearch#result', 'title': '中华人民共和国首都- 维基百科，自由的百科全书', 'htmlTitle': '中华人民共和国<b>首都</b>- 维基百科，自由的百科全书', 'link': 'https://zh.wikipedia.org/zh-hans/%E4%B8%AD%E5%8D%8E%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD%E9%A6%96%E9%83%BD', 'displayLink': 'zh.wikipedia.org', 'snippet': '中华人民共和国首都位于北京市，新中国成立前夕的旧称为北平，是中共中央及中央人民政府所在地，中央四个直辖市之一，全国政治、文化、国际交往和科技创新中心，中国古都、\\xa0...', 'htmlSnippet': '中华人民共和国<b>首都</b>位于北京市，新<b>中国</b>成立前夕的旧称为北平，是中共中央及中央人民政府所在地，中央四个直辖市之一，全国政治、文化、国际交往和科技创新中心，<b>中国</b>古都、&nbsp;...', 'formattedUrl': 'https://zh.wikipedia.org/zh-hans/中华人民共和国首都', 'htmlFormattedUrl': 'https://zh.wikipedia.org/zh-hans/中华人民共和国<b>首都</b>', 'pagemap': {'metatags': [{'referrer': 'origin', 'og:image': 'https://upload.wikimedia.org/wikipedia/commons/thumb/9/9a/Xinhuamen_Gate_of_Zhongnanhai_across_Changan_Street.JPG/1200px-Xinhuamen_Gate_of_Zhongnanhai_across_Changan_Street.JPG', 'theme-color': '#eaecf0', 'og:image:width': '1200', 'og:type': 'website', 'viewport': 'width=device-width, initial-scale=1.0, user-scalable=yes, minimum-scale=0.25, maximum-scale=5.0', 'og:title': '中华人民共和国首都 - 维基百科，自由的百科全书', 'og:image:height': '893', 'format-detection': 'telephone=no'}]}}, {'kind': 'customsearch#result', 'title': '中国历代首都_百度百科', 'htmlTitle': '<b>中国</b>历代<b>首都</b>_百度百科', 'link': 'https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E5%8E%86%E4%BB%A3%E9%A6%96%E9%83%BD/5737523', 'displayLink': 'baike.baidu.com', 'snippet': '首都是一个国家最重要的政治中心。在中国历史上，很多城市都成为过中国的首都。随着战争和朝代更迭不断的具有地域性的变迁，具有一定的经济、政治、文化等历史变化。', 'htmlSnippet': '首都是一个国家最重要的政治中心。在中国历史上，很多城市都成为过<b>中国的首都</b>。随着战争和朝代更迭不断的具有地域性的变迁，具有一定的经济、政治、文化等历史变化。', 'formattedUrl': 'https://baike.baidu.com/item/中国历代首都/5737523', 'htmlFormattedUrl': 'https://baike.baidu.com/item/<b>中国</b>历代<b>首都</b>/5737523', 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQHQWJuC_fcuZV2YcmZLVlOm6Cx5wK5Y_2nV0kr16JucUbm1T7frfpAcGw&s', 'width': '225', 'height': '225'}], 'metatags': [{'referrer': 'always', 'og:image': 'https://www.baidu.com/img/baidu.svg', 'image': 'https://bkimg.cdn.bcebos.com/smart/9213b07eca806538c4b8003998dda144ad34824e-bkimg-process,v_1,rw_1,rh_1,pad_1,color_ffffff?x-bce-process=image/format,f_auto', 'og:type': 'website', 'og:site_name': '百度百科', 'viewport': 'width=device-width,minimum-scale=1.0,maximum-scale=1.0,initial-scale=1.0,user-scalable=no', 'layoutmode': 'standard', 'apple-mobile-web-app-capable': 'yes', 'og:title': '百度百科', 'og:url': 'baike.baidu.com', 'og:description': '百度百科是一部内容开放、自由的网络百科全书，旨在创造一个涵盖所有领域知识，服务所有互联网用户的中文知识性百科全书。在这里你可以参与词条编辑，分享贡献你的知识。', 'format-detection': 'telephone=no,email=no'}], 'cse_image': [{'src': 'https://www.baidu.com/img/baidu.svg'}]}}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is set: True\n",
      "LANGSMITH_API_KEY is set: True\n",
      "GEMINI_API_KEY is : AIzaSyBPRZjGecVeND_KqHac8jeebFDzvlxFQhs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 打印一下关键环境变量，确认是否加载成功\n",
    "print(f\"GEMINI_API_KEY is set: {bool(os.getenv('GEMINI_API_KEY'))}\")\n",
    "print(f\"LANGSMITH_API_KEY is set: {bool(os.getenv('LANGSMITH_API_KEY'))}\") \n",
    "print(f\"GEMINI_API_KEY is : {os.getenv('GEMINI_API_KEY')}\") # 如果 LangSmith 也需要配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I help you today?' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4.1-2025-04-14', 'system_fingerprint': 'fp_07e970ab25'} id='run--54a153fb-7647-4a27-accb-3b0318a1b421-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import os, dotenv, pathlib\n",
    "\n",
    "# 加载 .env（若在 backend 目录，可写 dotenv.load_dotenv(\"backend/.env\")）\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# 创建 LLM 实例\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name='gpt-4.1',\n",
    "    model_name='gpt-4.1',\n",
    "    temperature=0.2,\n",
    "    streaming=True,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = llm.invoke([HumanMessage(content=\"hello\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Client in module google.genai.client object:\n",
      "\n",
      "class Client(builtins.object)\n",
      " |  Client(*, vertexai: Optional[bool] = None, api_key: Optional[str] = None, credentials: Optional[google.auth.credentials.Credentials] = None, project: Optional[str] = None, location: Optional[str] = None, debug_config: Optional[google.genai.client.DebugConfig] = None, http_options: Union[google.genai.types.HttpOptions, google.genai.types.HttpOptionsDict, NoneType] = None)\n",
      " |  \n",
      " |  Client for making synchronous requests.\n",
      " |  \n",
      " |  Use this client to make a request to the Gemini Developer API or Vertex AI\n",
      " |  API and then wait for the response.\n",
      " |  \n",
      " |  To initialize the client, provide the required arguments either directly\n",
      " |  or by using environment variables. Gemini API users and Vertex AI users in\n",
      " |  express mode can provide API key by providing input argument\n",
      " |  `api_key=\"your-api-key\"` or by defining `GOOGLE_API_KEY=\"your-api-key\"` as an\n",
      " |  environment variable\n",
      " |  \n",
      " |  Vertex AI API users can provide inputs argument as `vertexai=True,\n",
      " |  project=\"your-project-id\", location=\"us-central1\"` or by defining\n",
      " |  `GOOGLE_GENAI_USE_VERTEXAI=true`, `GOOGLE_CLOUD_PROJECT` and\n",
      " |  `GOOGLE_CLOUD_LOCATION` environment variables.\n",
      " |  \n",
      " |  Attributes:\n",
      " |    api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to\n",
      " |      use for authentication. Applies to the Gemini Developer API only.\n",
      " |    vertexai: Indicates whether the client should use the Vertex AI API\n",
      " |      endpoints. Defaults to False (uses Gemini Developer API endpoints).\n",
      " |      Applies to the Vertex AI API only.\n",
      " |    credentials: The credentials to use for authentication when calling the\n",
      " |      Vertex AI APIs. Credentials can be obtained from environment variables and\n",
      " |      default credentials. For more information, see `Set up Application Default\n",
      " |      Credentials\n",
      " |      <https://cloud.google.com/docs/authentication/provide-credentials-adc>`_.\n",
      " |      Applies to the Vertex AI API only.\n",
      " |    project: The `Google Cloud project ID\n",
      " |      <https://cloud.google.com/vertex-ai/docs/start/cloud-environment>`_ to use\n",
      " |      for quota. Can be obtained from environment variables (for example,\n",
      " |      ``GOOGLE_CLOUD_PROJECT``). Applies to the Vertex AI API only.\n",
      " |      Find your `Google Cloud project ID <https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects>`_.\n",
      " |    location: The `location\n",
      " |      <https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations>`_\n",
      " |      to send API requests to (for example, ``us-central1``). Can be obtained\n",
      " |      from environment variables. Applies to the Vertex AI API only.\n",
      " |    debug_config: Config settings that control network behavior of the client.\n",
      " |      This is typically used when running test code.\n",
      " |    http_options: Http options to use for the client. These options will be\n",
      " |      applied to all requests made by the client. Example usage: `client =\n",
      " |      genai.Client(http_options=types.HttpOptions(api_version='v1'))`.\n",
      " |  \n",
      " |  Usage for the Gemini Developer API:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |    from google import genai\n",
      " |  \n",
      " |    client = genai.Client(api_key='my-api-key')\n",
      " |  \n",
      " |  Usage for the Vertex AI API:\n",
      " |  \n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |    from google import genai\n",
      " |  \n",
      " |    client = genai.Client(\n",
      " |        vertexai=True, project='my-project-id', location='us-central1'\n",
      " |    )\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, vertexai: Optional[bool] = None, api_key: Optional[str] = None, credentials: Optional[google.auth.credentials.Credentials] = None, project: Optional[str] = None, location: Optional[str] = None, debug_config: Optional[google.genai.client.DebugConfig] = None, http_options: Union[google.genai.types.HttpOptions, google.genai.types.HttpOptionsDict, NoneType] = None)\n",
      " |      Initializes the client.\n",
      " |      \n",
      " |      Args:\n",
      " |         vertexai (bool): Indicates whether the client should use the Vertex AI\n",
      " |           API endpoints. Defaults to False (uses Gemini Developer API endpoints).\n",
      " |           Applies to the Vertex AI API only.\n",
      " |         api_key (str): The `API key\n",
      " |           <https://ai.google.dev/gemini-api/docs/api-key>`_ to use for\n",
      " |           authentication. Applies to the Gemini Developer API only.\n",
      " |         credentials (google.auth.credentials.Credentials): The credentials to use\n",
      " |           for authentication when calling the Vertex AI APIs. Credentials can be\n",
      " |           obtained from environment variables and default credentials. For more\n",
      " |           information, see `Set up Application Default Credentials\n",
      " |           <https://cloud.google.com/docs/authentication/provide-credentials-adc>`_.\n",
      " |           Applies to the Vertex AI API only.\n",
      " |         project (str): The `Google Cloud project ID\n",
      " |           <https://cloud.google.com/vertex-ai/docs/start/cloud-environment>`_ to\n",
      " |           use for quota. Can be obtained from environment variables (for example,\n",
      " |           ``GOOGLE_CLOUD_PROJECT``). Applies to the Vertex AI API only.\n",
      " |         location (str): The `location\n",
      " |           <https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations>`_\n",
      " |           to send API requests to (for example, ``us-central1``). Can be obtained\n",
      " |           from environment variables. Applies to the Vertex AI API only.\n",
      " |         debug_config (DebugConfig): Config settings that control network behavior\n",
      " |           of the client. This is typically used when running test code.\n",
      " |         http_options (Union[HttpOptions, HttpOptionsDict]): Http options to use\n",
      " |           for the client.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  aio\n",
      " |  \n",
      " |  auth_tokens\n",
      " |  \n",
      " |  batches\n",
      " |  \n",
      " |  caches\n",
      " |  \n",
      " |  chats\n",
      " |  \n",
      " |  files\n",
      " |  \n",
      " |  models\n",
      " |  \n",
      " |  operations\n",
      " |  \n",
      " |  tunings\n",
      " |  \n",
      " |  vertexai\n",
      " |      Returns whether the client is using the Vertex AI API.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.genai import Client\n",
    "\n",
    "genai_client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "help(genai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.DEBUG\n",
    ")\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields in Configuration class:\n",
      "- query_generator_model: (default: gemini-2.0-flash)\n",
      "- reflection_model: (default: gemini-2.5-flash-preview-04-17)\n",
      "- answer_model: (default: gemini-2.5-pro-preview-05-06)\n",
      "- openai_query_generator_model: (default: gpt-4o-mini)\n",
      "- openai_reflection_model: (default: gpt-4.1)\n",
      "- openai_answer_model: (default: gpt-4.1)\n",
      "- number_of_initial_queries: (default: 3)\n",
      "- max_research_loops: (default: 2)\n",
      "\n",
      "==================================================\n",
      "\n",
      "Created Configuration instance with current settings:\n",
      "{\n",
      "  \"query_generator_model\": \"gemini-2.0-flash\",\n",
      "  \"reflection_model\": \"gemini-2.5-flash-preview-04-17\",\n",
      "  \"answer_model\": \"gemini-2.5-pro-preview-05-06\",\n",
      "  \"openai_query_generator_model\": \"gpt-4o-mini\",\n",
      "  \"openai_reflection_model\": \"gpt-4.1\",\n",
      "  \"openai_answer_model\": \"gpt-4.1\",\n",
      "  \"number_of_initial_queries\": 3,\n",
      "  \"max_research_loops\": 2\n",
      "}\n",
      "\n",
      "==================================================\n",
      "\n",
      "Path from where 'src.agent.configuration' was loaded:\n",
      "d:\\mahc05\\gemini-fullstack-langgraph-quickstart\\backend\\src\\agent\\configuration.py\n"
     ]
    }
   ],
   "source": [
    "from src.agent.graph import graph\n",
    "from src.agent.configuration import Configuration\n",
    "import importlib\n",
    "import inspect\n",
    "\n",
    "# 重新载入模块以确保获取最新代码\n",
    "importlib.reload(importlib.import_module('src.agent.graph'))\n",
    "from src.agent.graph import graph\n",
    "\n",
    "# 打印 Configuration 类的所有字段及其默认值\n",
    "print(\"Fields in Configuration class:\")\n",
    "for field_name, field_obj in Configuration.model_fields.items():\n",
    "    print(f\"- {field_name}: (default: {field_obj.default})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 尝试创建一个 Configuration 实例并打印其内容\n",
    "# 这也会验证 from_runnable_config 是否能正常工作（如果环境变量未设置，则使用默认值）\n",
    "try:\n",
    "    config_instance = Configuration.from_runnable_config()\n",
    "    print(\"Created Configuration instance with current settings:\")\n",
    "    print(config_instance.model_dump_json(indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Configuration instance: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 检查模块的加载路径\n",
    "print(\"Path from where 'src.agent.configuration' was loaded:\")\n",
    "print(inspect.getfile(Configuration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'beta.chat.completions.stream', 'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-fa4dc469-0b07-4bd6-bb74-382003ca4815', 'json_data': {'messages': [{'content': 'Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\\n\\nInstructions:\\n- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\\n- Each query should focus on one specific aspect of the original question.\\n- Don\\'t produce more than 1 queries.\\n- Queries should be diverse, if the topic is broad, generate more than 1 query.\\n- Don\\'t generate multiple similar queries, 1 is enough.\\n- Query should ensure that the most current information is gathered. The current date is June 15, 2025.\\n\\nFormat: \\n- Format your response as a JSON object with ALL three of these exact keys:\\n   - \"rationale\": Brief explanation of why these queries are relevant\\n   - \"query\": A list of search queries\\n\\nExample:\\n\\nTopic: What revenue grew more last year apple stock or the number of people buying an iphone\\n```json\\n{\\n    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple\\'s stock performance and iPhone sales metrics. These queries target the precise financial information needed: company revenue trends, product-specific unit sales figures, and stock price movement over the same fiscal period for direct comparison.\",\\n    \"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\\n}\\n```\\n\\nContext: 中国的首都', 'role': 'user'}], 'model': 'gpt-4o-mini', 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'query': {'description': 'A list of search queries to be used for web research.', 'items': {'type': 'string'}, 'title': 'Query', 'type': 'array'}, 'rationale': {'description': 'A brief explanation of why these queries are relevant to the research topic.', 'title': 'Rationale', 'type': 'string'}}, 'required': ['query', 'rationale'], 'title': 'SearchQueryList', 'type': 'object', 'additionalProperties': False}, 'name': 'SearchQueryList', 'strict': True}}, 'stream': True, 'temperature': 1.0}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'beta.chat.completions.stream', 'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-fa4dc469-0b07-4bd6-bb74-382003ca4815', 'json_data': {'messages': [{'content': 'Your goal is to generate sophisticated and diverse web search queries. These queries are intended for an advanced automated web research tool capable of analyzing complex results, following links, and synthesizing information.\\n\\nInstructions:\\n- Always prefer a single search query, only add another query if the original question requests multiple aspects or elements and one query is not enough.\\n- Each query should focus on one specific aspect of the original question.\\n- Don\\'t produce more than 1 queries.\\n- Queries should be diverse, if the topic is broad, generate more than 1 query.\\n- Don\\'t generate multiple similar queries, 1 is enough.\\n- Query should ensure that the most current information is gathered. The current date is June 15, 2025.\\n\\nFormat: \\n- Format your response as a JSON object with ALL three of these exact keys:\\n   - \"rationale\": Brief explanation of why these queries are relevant\\n   - \"query\": A list of search queries\\n\\nExample:\\n\\nTopic: What revenue grew more last year apple stock or the number of people buying an iphone\\n```json\\n{\\n    \"rationale\": \"To answer this comparative growth question accurately, we need specific data points on Apple\\'s stock performance and iPhone sales metrics. These queries target the precise financial information needed: company revenue trends, product-specific unit sales figures, and stock price movement over the same fiscal period for direct comparison.\",\\n    \"query\": [\"Apple total revenue growth fiscal year 2024\", \"iPhone unit sales growth fiscal year 2024\", \"Apple stock price growth fiscal year 2024\"],\\n}\\n```\\n\\nContext: 中国的首都', 'role': 'user'}], 'model': 'gpt-4o-mini', 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'query': {'description': 'A list of search queries to be used for web research.', 'items': {'type': 'string'}, 'title': 'Query', 'type': 'array'}, 'rationale': {'description': 'A brief explanation of why these queries are relevant to the research topic.', 'title': 'Rationale', 'type': 'string'}}, 'required': ['query', 'rationale'], 'title': 'SearchQueryList', 'type': 'object', 'additionalProperties': False}, 'name': 'SearchQueryList', 'strict': True}}, 'stream': True, 'temperature': 1.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\n",
      "Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F8016C93D0>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F8016C93D0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F80163B2F0> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F80163B2F0> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B36050>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B36050>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Sun, 15 Jun 2025 07:58:00 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Sun, 15 Jun 2025 07:58:00 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "INFO:httpx:HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Sun, 15 Jun 2025 07:58:00 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Sun, 15 Jun 2025 07:58:00 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): customsearch.googleapis.com:443\n",
      "Starting new HTTPS connection (1): customsearch.googleapis.com:443\n",
      "DEBUG:urllib3.connectionpool:https://customsearch.googleapis.com:443 \"GET /customsearch/v1?key=AIzaSyAPHber3WnbmmPLd8PsSERkoWymIXdb3rU&cx=c5de16a7e3b3c4a93&q=Beijing+current+status+as+China%27s+capital+2025&num=5 HTTP/1.1\" 200 None\n",
      "https://customsearch.googleapis.com:443 \"GET /customsearch/v1?key=AIzaSyAPHber3WnbmmPLd8PsSERkoWymIXdb3rU&cx=c5de16a7e3b3c4a93&q=Beijing+current+status+as+China%27s+capital+2025&num=5 HTTP/1.1\" 200 None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'beta.chat.completions.parse', 'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-6a708b2b-ad64-44c3-981d-2e9c00779304', 'post_parser': <function Completions.parse.<locals>.parser at 0x000001F802B25580>, 'json_data': {'messages': [{'content': 'You are an expert research assistant analyzing summaries about \"中国的首都\".\\n\\nInstructions:\\n- Identify knowledge gaps or areas that need deeper exploration and generate a follow-up query. (1 or multiple).\\n- If provided summaries are sufficient to answer the user\\'s question, don\\'t generate a follow-up query.\\n- If there is a knowledge gap, generate a follow-up query that would help expand your understanding.\\n- Focus on technical details, implementation specifics, or emerging trends that weren\\'t fully covered.\\n\\nRequirements:\\n- Ensure the follow-up query is self-contained and includes necessary context for web search.\\n\\nOutput Format:\\n- Format your response as a JSON object with these exact keys:\\n   - \"is_sufficient\": true or false\\n   - \"knowledge_gap\": Describe what information is missing or needs clarification\\n   - \"follow_up_queries\": Write a specific question to address this gap\\n\\nExample:\\n```json\\n{\\n    \"is_sufficient\": true, // or false\\n    \"knowledge_gap\": \"The summary lacks information about performance metrics and benchmarks\", // \"\" if is_sufficient is true\\n    \"follow_up_queries\": [\"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\"] // [] if is_sufficient is true\\n}\\n```\\n\\nReflect carefully on the Summaries to identify knowledge gaps and produce a follow-up query. Then, produce your output following this JSON format:\\n\\nSummaries:\\nBeijing - Wikipedia: Beijing, previously romanized as Peking, is the capital city of China. With more than 22 million residents, it is the world\\'s most populous national capital\\xa0... (https://en.wikipedia.org/wiki/Beijing)\\nChina\\'s biggest state banks to raise $71.6 bln to boost capital | Reuters: Mar 30, 2025 ... BEIJING, March 30 (Reuters) - Four of China\\'s largest state-owned banks said on Sunday they plan to raise a combined 520 billion yuan\\xa0... (https://www.reuters.com/world/china/chinas-biggest-state-banks-raise-716-billion-private-placements-2025-03-30/)\\nChina - Wikipedia: Beijing is the country\\'s capital, while Shanghai is its most populous city by urban area and largest financial center. People\\'s Republic of China. 中华人民\\xa0... (https://en.wikipedia.org/wiki/China)\\nAfter the Fall: China\\'s Economy in 2025 – Rhodium Group: Dec 31, 2024 ... Beijing\\'s narrative of its own economic progress highlights certain economic data series (year-on-year real GDP growth, fixed asset investment,\\xa0... (https://rhg.com/research/after-the-fall-chinas-economy-in-2025/)\\nForum on China-Africa Cooperation Beijing Action Plan (2025-2027 ...: Sep 5, 2024 ... ... capital-intensive industries based on local conditions. China supports Chinese enterprises in expanding investment in Africa\\'s trade supply\\xa0... (https://www.mfa.gov.cn/eng/xw/zyxw/202409/t20240905_11485719.html)\\n', 'role': 'user'}], 'model': 'gpt-4.1', 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'is_sufficient': {'description': \"Whether the provided summaries are sufficient to answer the user's question.\", 'title': 'Is Sufficient', 'type': 'boolean'}, 'knowledge_gap': {'description': 'A description of what information is missing or needs clarification.', 'title': 'Knowledge Gap', 'type': 'string'}, 'follow_up_queries': {'description': 'A list of follow-up queries to address the knowledge gap.', 'items': {'type': 'string'}, 'title': 'Follow Up Queries', 'type': 'array'}}, 'required': ['is_sufficient', 'knowledge_gap', 'follow_up_queries'], 'title': 'Reflection', 'type': 'object', 'additionalProperties': False}, 'name': 'Reflection', 'strict': True}}, 'stream': False, 'temperature': 1.0}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'X-Stainless-Helper-Method': 'beta.chat.completions.parse', 'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-6a708b2b-ad64-44c3-981d-2e9c00779304', 'post_parser': <function Completions.parse.<locals>.parser at 0x000001F802B25580>, 'json_data': {'messages': [{'content': 'You are an expert research assistant analyzing summaries about \"中国的首都\".\\n\\nInstructions:\\n- Identify knowledge gaps or areas that need deeper exploration and generate a follow-up query. (1 or multiple).\\n- If provided summaries are sufficient to answer the user\\'s question, don\\'t generate a follow-up query.\\n- If there is a knowledge gap, generate a follow-up query that would help expand your understanding.\\n- Focus on technical details, implementation specifics, or emerging trends that weren\\'t fully covered.\\n\\nRequirements:\\n- Ensure the follow-up query is self-contained and includes necessary context for web search.\\n\\nOutput Format:\\n- Format your response as a JSON object with these exact keys:\\n   - \"is_sufficient\": true or false\\n   - \"knowledge_gap\": Describe what information is missing or needs clarification\\n   - \"follow_up_queries\": Write a specific question to address this gap\\n\\nExample:\\n```json\\n{\\n    \"is_sufficient\": true, // or false\\n    \"knowledge_gap\": \"The summary lacks information about performance metrics and benchmarks\", // \"\" if is_sufficient is true\\n    \"follow_up_queries\": [\"What are typical performance benchmarks and metrics used to evaluate [specific technology]?\"] // [] if is_sufficient is true\\n}\\n```\\n\\nReflect carefully on the Summaries to identify knowledge gaps and produce a follow-up query. Then, produce your output following this JSON format:\\n\\nSummaries:\\nBeijing - Wikipedia: Beijing, previously romanized as Peking, is the capital city of China. With more than 22 million residents, it is the world\\'s most populous national capital\\xa0... (https://en.wikipedia.org/wiki/Beijing)\\nChina\\'s biggest state banks to raise $71.6 bln to boost capital | Reuters: Mar 30, 2025 ... BEIJING, March 30 (Reuters) - Four of China\\'s largest state-owned banks said on Sunday they plan to raise a combined 520 billion yuan\\xa0... (https://www.reuters.com/world/china/chinas-biggest-state-banks-raise-716-billion-private-placements-2025-03-30/)\\nChina - Wikipedia: Beijing is the country\\'s capital, while Shanghai is its most populous city by urban area and largest financial center. People\\'s Republic of China. 中华人民\\xa0... (https://en.wikipedia.org/wiki/China)\\nAfter the Fall: China\\'s Economy in 2025 – Rhodium Group: Dec 31, 2024 ... Beijing\\'s narrative of its own economic progress highlights certain economic data series (year-on-year real GDP growth, fixed asset investment,\\xa0... (https://rhg.com/research/after-the-fall-chinas-economy-in-2025/)\\nForum on China-Africa Cooperation Beijing Action Plan (2025-2027 ...: Sep 5, 2024 ... ... capital-intensive industries based on local conditions. China supports Chinese enterprises in expanding investment in Africa\\'s trade supply\\xa0... (https://www.mfa.gov.cn/eng/xw/zyxw/202409/t20240905_11485719.html)\\n', 'role': 'user'}], 'model': 'gpt-4.1', 'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'is_sufficient': {'description': \"Whether the provided summaries are sufficient to answer the user's question.\", 'title': 'Is Sufficient', 'type': 'boolean'}, 'knowledge_gap': {'description': 'A description of what information is missing or needs clarification.', 'title': 'Knowledge Gap', 'type': 'string'}, 'follow_up_queries': {'description': 'A list of follow-up queries to address the knowledge gap.', 'items': {'type': 'string'}, 'title': 'Follow Up Queries', 'type': 'array'}}, 'required': ['is_sufficient', 'knowledge_gap', 'follow_up_queries'], 'title': 'Reflection', 'type': 'object', 'additionalProperties': False}, 'name': 'Reflection', 'strict': True}}, 'stream': False, 'temperature': 1.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview\n",
      "Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B36890>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B36890>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F802A2EB10> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F802A2EB10> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B62FD0>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B62FD0>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Sun, 15 Jun 2025 07:58:07 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'682'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Sun, 15 Jun 2025 07:58:07 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'682'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "INFO:httpx:HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Sun, 15 Jun 2025 07:58:07 GMT', 'content-type': 'application/json', 'content-length': '682', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Sun, 15 Jun 2025 07:58:07 GMT', 'content-type': 'application/json', 'content-length': '682', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "request_id: None\n",
      "DEBUG:openai._base_client:Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-fcd70139-25b0-4070-958f-57a391b63e9b', 'json_data': {'messages': [{'content': \"Generate a high-quality answer to the user's question based on the provided summaries.\\n\\nInstructions:\\n- The current date is June 15, 2025.\\n- You are the final step of a multi-step research process, don't mention that you are the final step. \\n- You have access to all the information gathered from the previous steps.\\n- You have access to the user's question.\\n- Generate a high-quality answer to the user's question based on the provided summaries and the user's question.\\n- you MUST include all the citations from the summaries in the answer correctly.\\n\\nUser Context:\\n- 中国的首都\\n\\nSummaries:\\nBeijing - Wikipedia: Beijing, previously romanized as Peking, is the capital city of China. With more than 22 million residents, it is the world's most populous national capital\\xa0... (https://en.wikipedia.org/wiki/Beijing)\\nChina's biggest state banks to raise $71.6 bln to boost capital | Reuters: Mar 30, 2025 ... BEIJING, March 30 (Reuters) - Four of China's largest state-owned banks said on Sunday they plan to raise a combined 520 billion yuan\\xa0... (https://www.reuters.com/world/china/chinas-biggest-state-banks-raise-716-billion-private-placements-2025-03-30/)\\nChina - Wikipedia: Beijing is the country's capital, while Shanghai is its most populous city by urban area and largest financial center. People's Republic of China. 中华人民\\xa0... (https://en.wikipedia.org/wiki/China)\\nAfter the Fall: China's Economy in 2025 – Rhodium Group: Dec 31, 2024 ... Beijing's narrative of its own economic progress highlights certain economic data series (year-on-year real GDP growth, fixed asset investment,\\xa0... (https://rhg.com/research/after-the-fall-chinas-economy-in-2025/)\\nForum on China-Africa Cooperation Beijing Action Plan (2025-2027 ...: Sep 5, 2024 ... ... capital-intensive industries based on local conditions. China supports Chinese enterprises in expanding investment in Africa's trade supply\\xa0... (https://www.mfa.gov.cn/eng/xw/zyxw/202409/t20240905_11485719.html)\", 'role': 'user'}], 'model': 'gpt-4.1', 'stream': True, 'temperature': 0.0}}\n",
      "Request options: {'method': 'post', 'url': '/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'idempotency_key': 'stainless-python-retry-fcd70139-25b0-4070-958f-57a391b63e9b', 'json_data': {'messages': [{'content': \"Generate a high-quality answer to the user's question based on the provided summaries.\\n\\nInstructions:\\n- The current date is June 15, 2025.\\n- You are the final step of a multi-step research process, don't mention that you are the final step. \\n- You have access to all the information gathered from the previous steps.\\n- You have access to the user's question.\\n- Generate a high-quality answer to the user's question based on the provided summaries and the user's question.\\n- you MUST include all the citations from the summaries in the answer correctly.\\n\\nUser Context:\\n- 中国的首都\\n\\nSummaries:\\nBeijing - Wikipedia: Beijing, previously romanized as Peking, is the capital city of China. With more than 22 million residents, it is the world's most populous national capital\\xa0... (https://en.wikipedia.org/wiki/Beijing)\\nChina's biggest state banks to raise $71.6 bln to boost capital | Reuters: Mar 30, 2025 ... BEIJING, March 30 (Reuters) - Four of China's largest state-owned banks said on Sunday they plan to raise a combined 520 billion yuan\\xa0... (https://www.reuters.com/world/china/chinas-biggest-state-banks-raise-716-billion-private-placements-2025-03-30/)\\nChina - Wikipedia: Beijing is the country's capital, while Shanghai is its most populous city by urban area and largest financial center. People's Republic of China. 中华人民\\xa0... (https://en.wikipedia.org/wiki/China)\\nAfter the Fall: China's Economy in 2025 – Rhodium Group: Dec 31, 2024 ... Beijing's narrative of its own economic progress highlights certain economic data series (year-on-year real GDP growth, fixed asset investment,\\xa0... (https://rhg.com/research/after-the-fall-chinas-economy-in-2025/)\\nForum on China-Africa Cooperation Beijing Action Plan (2025-2027 ...: Sep 5, 2024 ... ... capital-intensive industries based on local conditions. China supports Chinese enterprises in expanding investment in Africa's trade supply\\xa0... (https://www.mfa.gov.cn/eng/xw/zyxw/202409/t20240905_11485719.html)\", 'role': 'user'}], 'model': 'gpt-4.1', 'stream': True, 'temperature': 0.0}}\n",
      "DEBUG:openai._base_client:Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview\n",
      "Sending HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview\n",
      "DEBUG:httpcore.connection:connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "connect_tcp.started host='127.0.0.1' port=10809 local_address=None timeout=None socket_options=None\n",
      "DEBUG:httpcore.connection:connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B63890>\n",
      "connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F802B63890>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'CONNECT']>\n",
      "send_request_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'CONNECT']>\n",
      "send_request_body.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "receive_response_headers.started request=<Request [b'CONNECT']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'Connection established', [])\n",
      "DEBUG:httpcore.proxy:start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F802A2FAD0> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "start_tls.started ssl_context=<ssl.SSLContext object at 0x000001F802A2FAD0> server_hostname='tkcopilot.taikang.com' timeout=None\n",
      "DEBUG:httpcore.proxy:start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F8015CDE90>\n",
      "start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001F8015CDE90>\n",
      "DEBUG:httpcore.http11:send_request_headers.started request=<Request [b'POST']>\n",
      "send_request_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_headers.complete\n",
      "send_request_headers.complete\n",
      "DEBUG:httpcore.http11:send_request_body.started request=<Request [b'POST']>\n",
      "send_request_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:send_request_body.complete\n",
      "send_request_body.complete\n",
      "DEBUG:httpcore.http11:receive_response_headers.started request=<Request [b'POST']>\n",
      "receive_response_headers.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Sun, 15 Jun 2025 07:58:09 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Server', b'nginx/1.23.4'), (b'Date', b'Sun, 15 Jun 2025 07:58:09 GMT'), (b'Content-Type', b'text/event-stream; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Access-Control-Allow-Origin', b'*'), (b'Set-Cookie', b'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/'), (b'X-Version', b'0.6.10'), (b'X-Env', b'PRODUCTION')])\n",
      "INFO:httpx:HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "DEBUG:openai._base_client:HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Sun, 15 Jun 2025 07:58:09 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "HTTP Response: POST https://tkcopilot.taikang.com/openai/deployments/gpt-4.1/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'server': 'nginx/1.23.4', 'date': 'Sun, 15 Jun 2025 07:58:09 GMT', 'content-type': 'text/event-stream; charset=utf-8', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'set-cookie': 'remember_token=; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Path=/', 'x-version': '0.6.10', 'x-env': 'PRODUCTION'})\n",
      "DEBUG:openai._base_client:request_id: None\n",
      "request_id: None\n",
      "DEBUG:httpcore.http11:receive_response_body.started request=<Request [b'POST']>\n",
      "receive_response_body.started request=<Request [b'POST']>\n",
      "DEBUG:httpcore.http11:receive_response_body.complete\n",
      "receive_response_body.complete\n",
      "DEBUG:httpcore.http11:response_closed.started\n",
      "response_closed.started\n",
      "DEBUG:httpcore.http11:response_closed.complete\n",
      "response_closed.complete\n"
     ]
    }
   ],
   "source": [
    "state = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"中国的首都\"}], \"max_research_loops\": 3, \"initial_search_query_count\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyBPRZjGecVeND_KqHac8jeebFDzvlxFQhs\n",
      "INFO:google_genai.models:AFC is enabled with max remote calls: 10.\n",
      "AFC is enabled with max remote calls: 10.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Test a simple query\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(os.getenv(\u001b[33m\"\u001b[39m\u001b[33mGEMINI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mgenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-2.0-flash\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhello\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\models.py:6058\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   6056\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   6057\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m6058\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   6059\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   6060\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6061\u001b[39m   logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is done.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   6062\u001b[39m   remaining_remote_calls_afc -= \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\models.py:5007\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5004\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   5005\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m5007\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5008\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n\u001b[32m   5012\u001b[39m   response_dict = _GenerateContentResponse_from_vertex(\n\u001b[32m   5013\u001b[39m       \u001b[38;5;28mself\u001b[39m._api_client, response_dict\n\u001b[32m   5014\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\_api_client.py:927\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    918\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    919\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    923\u001b[39m ) -> Union[BaseResponse, Any]:\n\u001b[32m    924\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m    925\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m    926\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m   json_response = response.json\n\u001b[32m    929\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m json_response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\google\\genai\\_api_client.py:786\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m    782\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    783\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    784\u001b[39m   )\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m      \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m      \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m      \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m      \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m   errors.APIError.raise_for_response(response)\n\u001b[32m    794\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m    795\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m    796\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m         http2_negotiated = (\n\u001b[32m     82\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     kwargs = {\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    122\u001b[39m     }\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m         trace.return_value = stream\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\site-packages\\httpcore\\_backends\\sync.py:208\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     sock = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[32m    214\u001b[39m         sock.setsockopt(*option)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\Eliza\\anaconda3\\envs\\gemini_legacy\\Lib\\socket.py:855\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    854\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m855\u001b[39m         \u001b[43mexceptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[32m    856\u001b[39m     exceptions.append(exc)\n\u001b[32m    857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.genai import Client\n",
    "\n",
    "# Initialize the Gemini client\n",
    "genai_client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Test a simple query\n",
    "print(os.getenv(\"GEMINI_API_KEY\"))\n",
    "response = genai_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"hello\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pprint import pprint # For pretty printing dictionaries\n",
    "\n",
    "# 确保环境变量已加载 (特别是 GEMINI_API_KEY)\n",
    "if load_dotenv(): # 或者 load_dotenv(dotenv_path='.env')\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Could not load .env file, ensure GEMINI_API_KEY is set in your environment\")\n",
    "\n",
    "print(f\"GEMINI_API_KEY is set: {bool(os.getenv('GEMINI_API_KEY'))}\")\n",
    "print(f\"LANGSMITH_API_KEY is set: {bool(os.getenv('LANGSMITH_API_KEY'))}\")\n",
    "\n",
    "try:\n",
    "    from src.agent.graph import graph\n",
    "    print(\"Successfully imported 'graph' from src.agent.graph\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing graph: {e}\")\n",
    "    graph = None\n",
    "\n",
    "async def main():\n",
    "    if not graph:\n",
    "        print(\"Graph not loaded, skipping invocation.\")\n",
    "        return\n",
    "\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Who won the UEFA Euro 2024 football tournament?\"}\n",
    "        ],\n",
    "        \"max_research_loops\": 2,\n",
    "        \"initial_search_query_count\": 2\n",
    "    }\n",
    "\n",
    "    run_config = {\n",
    "        \"configurable\": {\n",
    "            # \"query_generator_model\": \"gemini-1.5-flash-latest\",\n",
    "            # \"reasoning_model\": \"gemini-1.5-pro-latest\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nStreaming agent execution log with initial state:\")\n",
    "    print(initial_state)\n",
    "    if run_config.get(\"configurable\"):\n",
    "        print(\"\\nAnd config:\")\n",
    "        print(run_config)\n",
    "    print(\"\\n--- Agent Log Start ---\")\n",
    "\n",
    "    try:\n",
    "        # 使用 astream_log() 来获取详细的执行日志\n",
    "        async for log_patch in graph.astream_log(\n",
    "            initial_state, \n",
    "            config=run_config, \n",
    "            include_names=None, \n",
    "            include_types=None, \n",
    "        ):\n",
    "            # RunLogPatch 对象包含一个 'ops' 列表，每个 op 是一个操作描述\n",
    "            for op in log_patch.ops:\n",
    "                print(f\"\\n[Log Operation: {op['op']}] Path: {op['path']}\")\n",
    "                if op['op'] == \"add\" or op['op'] == \"replace\":\n",
    "                    # 'value' 字段包含了添加或替换的数据\n",
    "                    print(\"  Value:\")\n",
    "                    pprint(op['value'], indent=2, width=120)\n",
    "                elif op['op'] == \"remove\":\n",
    "                    print(f\"  (Removed path: {op['path']})\")\n",
    "                # 可以根据 op['op'] 的类型 ('add', 'remove', 'replace') 和 op['path'] 来决定如何展示信息\n",
    "                # 例如，关心特定路径的更新：\n",
    "                # if op['path'] == \"/logs/OverallState/final_output\" and op['op'] == 'add':\n",
    "                #     print(\">>> Final Output Updated <<<\")\n",
    "                #     pprint(op['value'])\n",
    "                # if \"streamed_output\" in op['path'] and op['op'] == 'add': # LLM token stream\n",
    "                #     if isinstance(op['value'], list) and op['value'] and hasattr(op['value'][0], 'content'):\n",
    "                #         print(f\"  LLM Stream: {op['value'][0].content}\", end=\"\") # 假设是 AIMessageChunk\n",
    "                #     else:\n",
    "                #         pprint(op['value'], indent=2)\n",
    "\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        print(\"\\n--- Agent Log End ---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during agent streaming: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# **在 Notebook 单元格中，请直接运行:**\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({\"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": \"How has the most titles? List the top 5\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本，用于单独测试 generate_query 函数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.agent.graph import generate_query\n",
    "from src.agent.configuration import Configuration\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 创建测试状态\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建配置\n",
    "test_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"query_generator_model\": \"gemini-2.0-flash\",\n",
    "        \"reasoning_model\": \"gemini-2.0-pro\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 测试 generate_query 函数\n",
    "try:\n",
    "    print(\"\\n开始测试 generate_query 函数...\")\n",
    "    result = generate_query(test_state, test_config)\n",
    "    print(\"\\n测试成功!\")\n",
    "    print(f\"生成的查询: {result.get('search_query', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test script to directly test the Gemini API without importing from the project modules.\n",
    "This will help us verify if the issue is with our code changes or with the Gemini API itself.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key is set\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY is not set\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY is set\")\n",
    "\n",
    "# Import the Gemini API client\n",
    "try:\n",
    "    from google.genai import Client\n",
    "    print(\"Successfully imported google.genai\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing google.genai: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the Gemini API client\n",
    "genai_client = Client(api_key=gemini_api_key)\n",
    "\n",
    "# Test a simple query with system instruction for structured output\n",
    "try:\n",
    "    print(\"\\nTesting Gemini API with system instruction for structured output...\")\n",
    "    \n",
    "    # Define the system instruction for structured output\n",
    "    system_instruction = (\n",
    "        \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "        \"Return your response as a JSON object with the following structure: \"\n",
    "        \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "        \"where each query is a string that would be effective for web search.\"\n",
    "    )\n",
    "    \n",
    "    # Define the user prompt\n",
    "    user_prompt = \"Generate search queries for: Who won the UEFA Euro 2024 football tournament?\"\n",
    "    \n",
    "    # Set the generation config\n",
    "    generation_config = {\n",
    "        \"temperature\": 1.0,\n",
    "    }\n",
    "    \n",
    "    # Call the Gemini API\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[\n",
    "            {\"role\": \"system\", \"parts\": [system_instruction]},\n",
    "            {\"role\": \"user\", \"parts\": [user_prompt]}\n",
    "        ],\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    \n",
    "    # Print the response\n",
    "    print(f\"Response text: {response.text}\")\n",
    "    \n",
    "    # Try to parse the JSON response\n",
    "    try:\n",
    "        # Find the JSON part in the response\n",
    "        response_text = response.text\n",
    "        json_start = response_text.find('{')\n",
    "        json_end = response_text.rfind('}')\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_str = response_text[json_start:json_end+1]\n",
    "            data = json.loads(json_str)\n",
    "            print(f\"Parsed JSON: {json.dumps(data, indent=2)}\")\n",
    "        else:\n",
    "            print(\"No JSON found in response\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    \n",
    "    print(\"Test completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Gemini API: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "测试不同的 Gemini API 调用格式\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 初始化 Gemini 客户端\n",
    "#genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# 创建客户端\n",
    "client = genai.Client()\n",
    "# 测试不同的格式\n",
    "def test_format1():\n",
    "    \"\"\"测试格式1: 使用 Content 对象\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式1: 使用 Content 对象\")\n",
    "        \n",
    "        # 创建 Content 对象\n",
    "        content = genai.Content(\n",
    "            parts=[\n",
    "                genai.Part(text=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "            ],\n",
    "            role=\"user\"\n",
    "        )\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=content\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format2():\n",
    "    \"\"\"测试格式2: 使用字符串\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式2: 使用字符串\")\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=\"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format3():\n",
    "    \"\"\"测试格式3: 使用字典列表\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式3: 使用字典列表\")\n",
    "        \n",
    "        # 系统指令\n",
    "        system_instruction = (\n",
    "            \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "            \"Return your response as a JSON object with the following structure: \"\n",
    "            \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "            \"where each query is a string that would be effective for web search.\"\n",
    "        )\n",
    "        \n",
    "        # 用户提示\n",
    "        user_prompt = \"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                {\"role\": \"system\", \"parts\": [{\"text\": system_instruction}]},\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format4():\n",
    "    \"\"\"测试格式4: 使用 Part 对象列表\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式4: 使用 Part 对象列表\")\n",
    "        \n",
    "        # 系统指令\n",
    "        system_instruction = (\n",
    "            \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "            \"Return your response as a JSON object with the following structure: \"\n",
    "            \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "            \"where each query is a string that would be effective for web search.\"\n",
    "        )\n",
    "        \n",
    "        # 用户提示\n",
    "        user_prompt = \"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        \n",
    "        # 创建 Content 对象\n",
    "        system_content = genai.Content(\n",
    "            parts=[genai.Part(text=system_instruction)],\n",
    "            role=\"system\"\n",
    "        )\n",
    "        \n",
    "        user_content = genai.Content(\n",
    "            parts=[genai.Part(text=user_prompt)],\n",
    "            role=\"user\"\n",
    "        )\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[system_content, user_content]\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"开始测试不同的 Gemini API 调用格式...\")\n",
    "\n",
    "results = []\n",
    "results.append((\"格式1\", test_format1()))\n",
    "results.append((\"格式2\", test_format2()))\n",
    "results.append((\"格式3\", test_format3()))\n",
    "results.append((\"格式4\", test_format4()))\n",
    "\n",
    "print(\"\\n测试结果汇总:\")\n",
    "for format_name, success in results:\n",
    "    status = \"成功\" if success else \"失败\"\n",
    "    print(f\"{format_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本，用于单独测试 generate_query 函数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 导入要测试的函数\n",
    "from src.agent.graph import generate_query\n",
    "from src.agent.configuration import Configuration\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 创建测试状态\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个模拟的 Configuration 类\n",
    "class MockConfiguration:\n",
    "    def __init__(self):\n",
    "        self.query_generator_model = \"gemini-2.0-flash\"\n",
    "        self.reasoning_model = \"gemini-2.0-pro\"\n",
    "        self.number_of_initial_queries = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def from_runnable_config(cls, config):\n",
    "        return cls()\n",
    "\n",
    "# 创建一个模拟的 RunnableConfig 类\n",
    "class MockRunnableConfig:\n",
    "    def __init__(self):\n",
    "        self.configurable = MockConfiguration()\n",
    "\n",
    "# 测试 generate_query 函数\n",
    "try:\n",
    "    print(\"\\n开始测试 generate_query 函数...\")\n",
    "    print(\"\\n使用的模型: gemini-2.0-flash\")\n",
    "    print(\"\\n测试状态: \", test_state)\n",
    "    \n",
    "    # 添加调试信息\n",
    "    print(\"\\n开始调用 generate_query...\")\n",
    "    \n",
    "    # 创建模拟的配置对象\n",
    "    test_config = MockRunnableConfig()\n",
    "    \n",
    "    # 调用函数\n",
    "    result = generate_query(test_state, test_config)\n",
    "    print(\"\\n测试成功!\")\n",
    "    print(f\"生成的查询: {result.get('query_list', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os, dotenv\n",
    "\n",
    "# 加载 .env（若在 backend 目录，可写 dotenv.load_dotenv(\"backend/.env\")）\n",
    "dotenv.load_dotenv()\n",
    "# 创建 LLM 实例\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name='gpt-4.1',\n",
    "    model_name='gpt-4.1',\n",
    "    temperature=0.2,\n",
    "    streaming=True,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = llm.invoke([HumanMessage(content=\"hello\")])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20^4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194481"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#20的四次方\n",
    "21**4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=22, 234256, sum1=121, sum2=121\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    if i**4 >99999 and i**4 < 1000000:\n",
    "        # 得到的6尾数 abcdef 计算ab+cd+ef 以及ba+dc+fe\n",
    "        num = i**4\n",
    "        num_str = str(num)\n",
    "        a, b = int(num_str[-6]), int(num_str[-5])\n",
    "        c, d = int(num_str[-4]), int(num_str[-3])\n",
    "        e, f = int(num_str[-2]), int(num_str[-1])\n",
    "        sum1 = 10*a + b + 10*c + d + 10*e + f\n",
    "        sum2 = 10*b + a + 10*d + c + 10*f + e\n",
    "        if sum1 == sum2 and int(sum1**0.5)**2 == sum1:\n",
    "            print(f\"i={i}, {num}, sum1={sum1}, sum2={sum2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
