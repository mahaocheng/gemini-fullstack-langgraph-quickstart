{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# 确保环境变量已加载 (特别是 GEMINI_API_KEY)\n",
    "# 假设 .env 文件位于 notebook 所在的 backend 目录的上一级 (即项目根目录)\n",
    "# 如果 .env 在 backend 目录中，路径应为 '.env' 或 '../.env' (如果notebook在src/agent下)\n",
    "# 根据您的 .env 文件实际位置调整 dotenv_path\n",
    "# 您之前提到 .env 在 backend 目录中，所以应该是 load_dotenv() 或 load_dotenv(dotenv_path='.env')\n",
    "# 如果 .env 在项目根目录，则是 load_dotenv(dotenv_path='../.env')\n",
    "# 从您打开的文件看，.env 在 backend 目录，所以 load_dotenv() 应该可以。\n",
    "if load_dotenv(): # 或者 load_dotenv(dotenv_path='.env')\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Could not load .env file, ensure GEMINI_API_KEY is set in your environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "#help(StateGraph)\n",
    "from langchain_core.messages import AIMessage\n",
    "help(AIMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import os\n",
    "\n",
    "DEFAULT_SEARCH_ENGINE_TIMEOUT = 100\n",
    "REFERENCE_COUNT = 5\n",
    "GOOGLE_SEARCH_ENDPOINT = \"https://customsearch.googleapis.com/customsearch/v1\"\n",
    "\n",
    "def search_with_google(query: str, subscription_key: str, cx: str):\n",
    "    \"\"\"\n",
    "    Search with google and return the contexts.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"key\": subscription_key,\n",
    "        \"cx\": cx,\n",
    "        \"q\": query,\n",
    "        \"num\": REFERENCE_COUNT,\n",
    "    }\n",
    "    response = requests.get(\n",
    "        GOOGLE_SEARCH_ENDPOINT, params=params, timeout=DEFAULT_SEARCH_ENGINE_TIMEOUT\n",
    "    )\n",
    "    if not response.ok:\n",
    "        print(f\"{response.status_code} {response.text}\")\n",
    "        raise HTTPException(response.status_code, \"Search engine error.\")\n",
    "    json_content = response.json()\n",
    "    try:\n",
    "        contexts = json_content[\"items\"][:REFERENCE_COUNT]\n",
    "    except KeyError:\n",
    "        print(f\"Error encountered: {json_content}\")\n",
    "        return []\n",
    "    return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_api_key = os.environ[\"GOOGLE_SEARCH_API_KEY\"]\n",
    "# with DDGS() as ddgs:\n",
    "#     print(query, \"xxx\")\n",
    "#     results = list(ddgs.text(query, max_results=5))\n",
    "\n",
    "results = search_with_google(\"中国的首都\", search_api_key, \n",
    "os.environ[\"GOOGLE_SEARCH_CX\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 打印一下关键环境变量，确认是否加载成功\n",
    "print(f\"GEMINI_API_KEY is set: {bool(os.getenv('GEMINI_API_KEY'))}\")\n",
    "print(f\"LANGSMITH_API_KEY is set: {bool(os.getenv('LANGSMITH_API_KEY'))}\") \n",
    "print(f\"GEMINI_API_KEY is : {os.getenv('GEMINI_API_KEY')}\") # 如果 LangSmith 也需要配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "import os, dotenv, pathlib\n",
    "\n",
    "# 加载 .env（若在 backend 目录，可写 dotenv.load_dotenv(\"backend/.env\")）\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "# 创建 LLM 实例\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name='gpt-4.1',\n",
    "    model_name='gpt-4.1',\n",
    "    temperature=0.2,\n",
    "    streaming=True,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = llm.invoke([HumanMessage(content=\"hello\")])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import Client\n",
    "\n",
    "genai_client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "help(genai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from IPython.display import display, HTML\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    stream=sys.stdout, level=logging.DEBUG\n",
    ")\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent.graph import graph\n",
    "from src.agent.configuration import Configuration\n",
    "import importlib\n",
    "import inspect\n",
    "\n",
    "# 重新载入模块以确保获取最新代码\n",
    "importlib.reload(importlib.import_module('src.agent.graph'))\n",
    "from src.agent.graph import graph\n",
    "\n",
    "# 打印 Configuration 类的所有字段及其默认值\n",
    "print(\"Fields in Configuration class:\")\n",
    "for field_name, field_obj in Configuration.model_fields.items():\n",
    "    print(f\"- {field_name}: (default: {field_obj.default})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 尝试创建一个 Configuration 实例并打印其内容\n",
    "# 这也会验证 from_runnable_config 是否能正常工作（如果环境变量未设置，则使用默认值）\n",
    "try:\n",
    "    config_instance = Configuration.from_runnable_config()\n",
    "    print(\"Created Configuration instance with current settings:\")\n",
    "    print(config_instance.model_dump_json(indent=2))\n",
    "except Exception as e:\n",
    "    print(f\"Error creating Configuration instance: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 检查模块的加载路径\n",
    "print(\"Path from where 'src.agent.configuration' was loaded:\")\n",
    "print(inspect.getfile(Configuration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"中国的首都\"}], \"max_research_loops\": 3, \"initial_search_query_count\": 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.genai import Client\n",
    "\n",
    "# Initialize the Gemini client\n",
    "genai_client = Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Test a simple query\n",
    "print(os.getenv(\"GEMINI_API_KEY\"))\n",
    "response = genai_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=\"hello\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pprint import pprint # For pretty printing dictionaries\n",
    "\n",
    "# 确保环境变量已加载 (特别是 GEMINI_API_KEY)\n",
    "if load_dotenv(): # 或者 load_dotenv(dotenv_path='.env')\n",
    "    print(\"Loaded .env file\")\n",
    "else:\n",
    "    print(\"Could not load .env file, ensure GEMINI_API_KEY is set in your environment\")\n",
    "\n",
    "print(f\"GEMINI_API_KEY is set: {bool(os.getenv('GEMINI_API_KEY'))}\")\n",
    "print(f\"LANGSMITH_API_KEY is set: {bool(os.getenv('LANGSMITH_API_KEY'))}\")\n",
    "\n",
    "try:\n",
    "    from src.agent.graph import graph\n",
    "    print(\"Successfully imported 'graph' from src.agent.graph\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing graph: {e}\")\n",
    "    graph = None\n",
    "\n",
    "async def main():\n",
    "    if not graph:\n",
    "        print(\"Graph not loaded, skipping invocation.\")\n",
    "        return\n",
    "\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Who won the UEFA Euro 2024 football tournament?\"}\n",
    "        ],\n",
    "        \"max_research_loops\": 2,\n",
    "        \"initial_search_query_count\": 2\n",
    "    }\n",
    "\n",
    "    run_config = {\n",
    "        \"configurable\": {\n",
    "            # \"query_generator_model\": \"gemini-1.5-flash-latest\",\n",
    "            # \"reasoning_model\": \"gemini-1.5-pro-latest\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\nStreaming agent execution log with initial state:\")\n",
    "    print(initial_state)\n",
    "    if run_config.get(\"configurable\"):\n",
    "        print(\"\\nAnd config:\")\n",
    "        print(run_config)\n",
    "    print(\"\\n--- Agent Log Start ---\")\n",
    "\n",
    "    try:\n",
    "        # 使用 astream_log() 来获取详细的执行日志\n",
    "        async for log_patch in graph.astream_log(\n",
    "            initial_state, \n",
    "            config=run_config, \n",
    "            include_names=None, \n",
    "            include_types=None, \n",
    "        ):\n",
    "            # RunLogPatch 对象包含一个 'ops' 列表，每个 op 是一个操作描述\n",
    "            for op in log_patch.ops:\n",
    "                print(f\"\\n[Log Operation: {op['op']}] Path: {op['path']}\")\n",
    "                if op['op'] == \"add\" or op['op'] == \"replace\":\n",
    "                    # 'value' 字段包含了添加或替换的数据\n",
    "                    print(\"  Value:\")\n",
    "                    pprint(op['value'], indent=2, width=120)\n",
    "                elif op['op'] == \"remove\":\n",
    "                    print(f\"  (Removed path: {op['path']})\")\n",
    "                # 可以根据 op['op'] 的类型 ('add', 'remove', 'replace') 和 op['path'] 来决定如何展示信息\n",
    "                # 例如，关心特定路径的更新：\n",
    "                # if op['path'] == \"/logs/OverallState/final_output\" and op['op'] == 'add':\n",
    "                #     print(\">>> Final Output Updated <<<\")\n",
    "                #     pprint(op['value'])\n",
    "                # if \"streamed_output\" in op['path'] and op['op'] == 'add': # LLM token stream\n",
    "                #     if isinstance(op['value'], list) and op['value'] and hasattr(op['value'][0], 'content'):\n",
    "                #         print(f\"  LLM Stream: {op['value'][0].content}\", end=\"\") # 假设是 AIMessageChunk\n",
    "                #     else:\n",
    "                #         pprint(op['value'], indent=2)\n",
    "\n",
    "\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        print(\"\\n--- Agent Log End ---\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred during agent streaming: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# **在 Notebook 单元格中，请直接运行:**\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.invoke({\"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": \"How has the most titles? List the top 5\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本，用于单独测试 generate_query 函数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from src.agent.graph import generate_query\n",
    "from src.agent.configuration import Configuration\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 创建测试状态\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建配置\n",
    "test_config = RunnableConfig(\n",
    "    configurable={\n",
    "        \"query_generator_model\": \"gemini-2.0-flash\",\n",
    "        \"reasoning_model\": \"gemini-2.0-pro\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 测试 generate_query 函数\n",
    "try:\n",
    "    print(\"\\n开始测试 generate_query 函数...\")\n",
    "    result = generate_query(test_state, test_config)\n",
    "    print(\"\\n测试成功!\")\n",
    "    print(f\"生成的查询: {result.get('search_query', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test script to directly test the Gemini API without importing from the project modules.\n",
    "This will help us verify if the issue is with our code changes or with the Gemini API itself.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check if API key is set\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY is not set\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY is set\")\n",
    "\n",
    "# Import the Gemini API client\n",
    "try:\n",
    "    from google.genai import Client\n",
    "    print(\"Successfully imported google.genai\")\n",
    "except Exception as e:\n",
    "    print(f\"Error importing google.genai: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize the Gemini API client\n",
    "genai_client = Client(api_key=gemini_api_key)\n",
    "\n",
    "# Test a simple query with system instruction for structured output\n",
    "try:\n",
    "    print(\"\\nTesting Gemini API with system instruction for structured output...\")\n",
    "    \n",
    "    # Define the system instruction for structured output\n",
    "    system_instruction = (\n",
    "        \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "        \"Return your response as a JSON object with the following structure: \"\n",
    "        \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "        \"where each query is a string that would be effective for web search.\"\n",
    "    )\n",
    "    \n",
    "    # Define the user prompt\n",
    "    user_prompt = \"Generate search queries for: Who won the UEFA Euro 2024 football tournament?\"\n",
    "    \n",
    "    # Set the generation config\n",
    "    generation_config = {\n",
    "        \"temperature\": 1.0,\n",
    "    }\n",
    "    \n",
    "    # Call the Gemini API\n",
    "    response = genai_client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=[\n",
    "            {\"role\": \"system\", \"parts\": [system_instruction]},\n",
    "            {\"role\": \"user\", \"parts\": [user_prompt]}\n",
    "        ],\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    \n",
    "    # Print the response\n",
    "    print(f\"Response text: {response.text}\")\n",
    "    \n",
    "    # Try to parse the JSON response\n",
    "    try:\n",
    "        # Find the JSON part in the response\n",
    "        response_text = response.text\n",
    "        json_start = response_text.find('{')\n",
    "        json_end = response_text.rfind('}')\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_str = response_text[json_start:json_end+1]\n",
    "            data = json.loads(json_str)\n",
    "            print(f\"Parsed JSON: {json.dumps(data, indent=2)}\")\n",
    "        else:\n",
    "            print(\"No JSON found in response\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "    \n",
    "    print(\"Test completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error testing Gemini API: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "测试不同的 Gemini API 调用格式\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 初始化 Gemini 客户端\n",
    "#genai.configure(api_key=gemini_api_key)\n",
    "\n",
    "# 创建客户端\n",
    "client = genai.Client()\n",
    "# 测试不同的格式\n",
    "def test_format1():\n",
    "    \"\"\"测试格式1: 使用 Content 对象\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式1: 使用 Content 对象\")\n",
    "        \n",
    "        # 创建 Content 对象\n",
    "        content = genai.Content(\n",
    "            parts=[\n",
    "                genai.Part(text=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "            ],\n",
    "            role=\"user\"\n",
    "        )\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=content\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format2():\n",
    "    \"\"\"测试格式2: 使用字符串\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式2: 使用字符串\")\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=\"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format3():\n",
    "    \"\"\"测试格式3: 使用字典列表\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式3: 使用字典列表\")\n",
    "        \n",
    "        # 系统指令\n",
    "        system_instruction = (\n",
    "            \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "            \"Return your response as a JSON object with the following structure: \"\n",
    "            \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "            \"where each query is a string that would be effective for web search.\"\n",
    "        )\n",
    "        \n",
    "        # 用户提示\n",
    "        user_prompt = \"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[\n",
    "                {\"role\": \"system\", \"parts\": [{\"text\": system_instruction}]},\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": user_prompt}]}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def test_format4():\n",
    "    \"\"\"测试格式4: 使用 Part 对象列表\"\"\"\n",
    "    try:\n",
    "        print(\"\\n测试格式4: 使用 Part 对象列表\")\n",
    "        \n",
    "        # 系统指令\n",
    "        system_instruction = (\n",
    "            \"You are a search query generator. Generate search queries based on the user's research topic. \"\n",
    "            \"Return your response as a JSON object with the following structure: \"\n",
    "            \"{ \\\"query\\\": [\\\"query1\\\", \\\"query2\\\", ...] } \"\n",
    "            \"where each query is a string that would be effective for web search.\"\n",
    "        )\n",
    "        \n",
    "        # 用户提示\n",
    "        user_prompt = \"谁赢得了2024年欧洲杯足球赛?\"\n",
    "        \n",
    "        # 创建 Content 对象\n",
    "        system_content = genai.Content(\n",
    "            parts=[genai.Part(text=system_instruction)],\n",
    "            role=\"system\"\n",
    "        )\n",
    "        \n",
    "        user_content = genai.Content(\n",
    "            parts=[genai.Part(text=user_prompt)],\n",
    "            role=\"user\"\n",
    "        )\n",
    "        \n",
    "        # 调用 API\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            contents=[system_content, user_content]\n",
    "        )\n",
    "        \n",
    "        print(\"成功! 响应:\", response.text[:100] + \"...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"开始测试不同的 Gemini API 调用格式...\")\n",
    "\n",
    "results = []\n",
    "results.append((\"格式1\", test_format1()))\n",
    "results.append((\"格式2\", test_format2()))\n",
    "results.append((\"格式3\", test_format3()))\n",
    "results.append((\"格式4\", test_format4()))\n",
    "\n",
    "print(\"\\n测试结果汇总:\")\n",
    "for format_name, success in results:\n",
    "    status = \"成功\" if success else \"失败\"\n",
    "    print(f\"{format_name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "测试脚本，用于单独测试 generate_query 函数\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 导入要测试的函数\n",
    "from src.agent.graph import generate_query\n",
    "from src.agent.configuration import Configuration\n",
    "\n",
    "# 加载环境变量\n",
    "load_dotenv()\n",
    "\n",
    "# 检查API密钥是否设置\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if gemini_api_key is None:\n",
    "    print(\"GEMINI_API_KEY 未设置\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY 已设置\")\n",
    "\n",
    "# 创建测试状态\n",
    "test_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"谁赢得了2024年欧洲杯足球赛?\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个模拟的 Configuration 类\n",
    "class MockConfiguration:\n",
    "    def __init__(self):\n",
    "        self.query_generator_model = \"gemini-2.0-flash\"\n",
    "        self.reasoning_model = \"gemini-2.0-pro\"\n",
    "        self.number_of_initial_queries = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def from_runnable_config(cls, config):\n",
    "        return cls()\n",
    "\n",
    "# 创建一个模拟的 RunnableConfig 类\n",
    "class MockRunnableConfig:\n",
    "    def __init__(self):\n",
    "        self.configurable = MockConfiguration()\n",
    "\n",
    "# 测试 generate_query 函数\n",
    "try:\n",
    "    print(\"\\n开始测试 generate_query 函数...\")\n",
    "    print(\"\\n使用的模型: gemini-2.0-flash\")\n",
    "    print(\"\\n测试状态: \", test_state)\n",
    "    \n",
    "    # 添加调试信息\n",
    "    print(\"\\n开始调用 generate_query...\")\n",
    "    \n",
    "    # 创建模拟的配置对象\n",
    "    test_config = MockRunnableConfig()\n",
    "    \n",
    "    # 调用函数\n",
    "    result = generate_query(test_state, test_config)\n",
    "    print(\"\\n测试成功!\")\n",
    "    print(f\"生成的查询: {result.get('query_list', [])}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n测试失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "import os, dotenv\n",
    "\n",
    "# 加载 .env（若在 backend 目录，可写 dotenv.load_dotenv(\"backend/.env\")）\n",
    "dotenv.load_dotenv()\n",
    "# 创建 LLM 实例\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name='gpt-4.1',\n",
    "    model_name='gpt-4.1',\n",
    "    temperature=0.2,\n",
    "    streaming=False,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "response = llm.invoke([HumanMessage(content=\"hello\")])\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph\n",
    "from typing import TypedDict, Annotated, Literal\n",
    "from langgraph.graph import END\n",
    "import operator\n",
    "import json\n",
    "\n",
    "# 定义状态\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list, operator.add]\n",
    "    next_step: str\n",
    "\n",
    "# 初始化LLM\n",
    "#llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# 路由节点\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"确定用户的意图并路由到相应的专家。如果用户在请求信息，路由到'information'。如果用户正在请求创建内容，路由到'creation'.\"),\n",
    "    (\"placeholder\", \"{messages}\")\n",
    "])\n",
    "\n",
    "def router(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1].content\n",
    "    \n",
    "    # 简单判断，不依赖复杂的函数调用\n",
    "    # 这只是一个简单示例，实际中可能需要更复杂的逻辑\n",
    "    keywords_for_creation = [\"写\", \"创造\", \"故事\", \"创作\", \"生成\"]\n",
    "    \n",
    "    # 检查是否包含创作相关关键词\n",
    "    is_creation = any(keyword in last_message for keyword in keywords_for_creation)\n",
    "    \n",
    "    # 确定路由\n",
    "    destination = \"creation\" if is_creation else \"information\"\n",
    "    \n",
    "    print(f\"路由决策: {destination}, 基于消息: {last_message[:30]}...\")\n",
    "    return {\"next_step\": destination}\n",
    "\n",
    "# 信息专家\n",
    "def information_expert(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    information_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个信息专家，能够提供准确、有用的信息。\"),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ])\n",
    "    \n",
    "    # 直接在invoke中提供所有参数\n",
    "    response = information_prompt.pipe(llm).invoke({\"messages\": messages})\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "\n",
    "def creation_expert(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    \n",
    "    creation_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个创意专家，能够生成原创内容。\"),\n",
    "        (\"placeholder\", \"{messages}\")\n",
    "    ])\n",
    "    \n",
    "    # 直接在invoke中提供所有参数\n",
    "    response = creation_prompt.pipe(llm).invoke({\"messages\": messages})\n",
    "    return {\"messages\": [AIMessage(content=response.content)]}\n",
    "def should_continue(state):\n",
    "    # 返回字典，而不是直接返回字符串\n",
    "    if len(state[\"messages\"]) >= 3:  # 假设最多3轮对话\n",
    "        return {\"next_step\": \"end\"}\n",
    "    else:\n",
    "        return {\"next_step\": \"router\"}\n",
    "\n",
    "# 创建图形\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# 添加节点\n",
    "graph.add_node(\"router\", router)\n",
    "graph.add_node(\"information\", information_expert)\n",
    "graph.add_node(\"creation\", creation_expert)\n",
    "graph.add_node(\"should_continue\", should_continue)  # 添加判断节点\n",
    "# 添加从router到专家节点的条件边\n",
    "\n",
    "# 添加边和条件路由\n",
    "graph.set_entry_point(\"router\")\n",
    "graph.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: state[\"next_step\"],\n",
    "    {\n",
    "        \"information\": \"information\",\n",
    "        \"creation\": \"creation\"\n",
    "    }\n",
    ")\n",
    "graph.add_conditional_edges(\n",
    "    \"should_continue\",\n",
    "    lambda state: state[\"next_step\"],  # 从返回的字典中获取next_step字段\n",
    "    {\n",
    "        \"router\": \"router\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"information\", \"should_continue\")  # 使用节点名称而不是函数\n",
    "graph.add_edge(\"creation\", \"should_continue\")  # 使用节点名称而不是函数\n",
    "\n",
    "\n",
    "# 编译图形\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始状态: {'messages': [HumanMessage(content='为我写一个短故事', additional_kwargs={}, response_metadata={})], 'next_step': ''}\n",
      "路由决策: creation, 基于消息: 为我写一个短故事...\n",
      "路由决策: information, 基于消息: 在一个遥远的小镇上，住着一个爱做梦的男孩小宇。他每天放学后，...\n",
      "结果: {'messages': [HumanMessage(content='为我写一个短故事', additional_kwargs={}, response_metadata={}), AIMessage(content='在一个遥远的小镇上，住着一个爱做梦的男孩小宇。他每天放学后，都会跑到镇外的老橡树下，仰望天空，幻想自己能飞上云端。\\n\\n有一天，小宇在树下发现了一只受伤的小鸟。他小心翼翼地把小鸟带回家，细心照料。几天后，小鸟渐渐恢复了健康。小宇舍不得让它离开，但他知道，鸟儿属于天空。\\n\\n终于，小宇鼓起勇气，把小鸟带到橡树下，轻声说：“去吧，飞向你的世界。”小鸟在他手心停留片刻，仿佛在道谢，然后振翅高飞。\\n\\n小宇望着小鸟消失在云端，心里充满了温暖。他明白了一个道理：有时候，放手也是一种美丽的成全。从那天起，小宇的梦想不再只是飞翔，而是学会了珍惜和成长。', additional_kwargs={}, response_metadata={}), AIMessage(content='在一个安静的小村庄里，住着一位叫阿明的老人。他每天清晨都会在村头的小河边散步，和河里的鸭子们打招呼。\\n\\n有一天，阿明发现河边多了一只小鸭子，它孤零零地站在石头上，显得很害怕。阿明轻声安慰它，把它带回了家。几天后，小鸭子变得活泼起来，还学会了跟着阿明散步。\\n\\n春天过去，夏天来临。一天早晨，小鸭子的妈妈带着一群鸭子游到了河边。小鸭子看到妈妈，高兴地扑进了水里。阿明微笑着目送它们远去，心里既有不舍，也有满足。\\n\\n从那以后，阿明每天散步时，总能看到那只小鸭子在河里向他挥动翅膀。他明白，善良和陪伴，会在心里留下温暖的回忆。', additional_kwargs={}, response_metadata={})], 'next_step': 'end'}\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 定义正确的测试数据\n",
    "# 定义正确的测试数据\n",
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"为我写一个短故事\")],\n",
    "    \"next_step\": \"\"  # 或者您可以直接设置为 \"router\"\n",
    "}\n",
    "\n",
    "# 打印初始状态，便于调试\n",
    "print(\"初始状态:\", state)\n",
    "result = app.invoke(state)\n",
    "print(\"结果:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 运行图形\n",
    "try:\n",
    "    result = app.invoke(state)\n",
    "    print(\"结果:\", result)\n",
    "    \n",
    "    # 如果测试成功，尝试第二轮对话\n",
    "    if \"messages\" in result:\n",
    "        print(\"AI回复:\", result[\"messages\"][-1].content)\n",
    "        \n",
    "        # 添加用户回复\n",
    "        result[\"messages\"].append(HumanMessage(content=\"让故事更有趣一些\"))\n",
    "        \n",
    "        # 运行第二轮\n",
    "        result2 = app.invoke(result)\n",
    "        print(\"\\n第二轮AI回复:\", result2[\"messages\"][-1].content)\n",
    "except Exception as e:\n",
    "    print(f\"错误类型: {type(e)}\")\n",
    "    print(f\"错误信息: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    if i**4 >99999 and i**4 < 1000000:\n",
    "        # 得到的6尾数 abcdef 计算ab+cd+ef 以及ba+dc+fe\n",
    "        num = i**4\n",
    "        num_str = str(num)\n",
    "        a, b = int(num_str[-6]), int(num_str[-5])\n",
    "        c, d = int(num_str[-4]), int(num_str[-3])\n",
    "        e, f = int(num_str[-2]), int(num_str[-1])\n",
    "        sum1 = 10*a + b + 10*c + d + 10*e + f\n",
    "        sum2 = 10*b + a + 10*d + c + 10*f + e\n",
    "        if sum1 == sum2 and int(sum1**0.5)**2 == sum1:\n",
    "            print(f\"i={i}, {num}, sum1={sum1}, sum2={sum2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini_legacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
